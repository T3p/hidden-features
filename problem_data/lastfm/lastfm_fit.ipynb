{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spanrd(vectors, d):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - vectors (array): matrix (N, d)\n",
    "        - d (int): dimension of the space to be spanned\n",
    "    Return:\n",
    "        - True or False\n",
    "    \"\"\"\n",
    "    # https://math.stackexchange.com/questions/56201/how-to-tell-if-a-set-of-vectors-spans-a-space\n",
    "    # https://stackoverflow.com/questions/15638650/is-there-a-standard-solution-for-gauss-elimination-in-python\n",
    "    pl, u = lu(vectors, permute_l=True)\n",
    "    rank = np.linalg.matrix_rank(u)\n",
    "    return d == int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (1322, 220)\n",
      "ratings: max 6.648947490095273 - min -0.3259578617346158\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data_path = \"lastfmlog.npy\"\n",
    "\n",
    "ratings = np.load(data_path)\n",
    "#print(np.mean(ratings), np.sum(ratings > 0), ratings.size)\n",
    "ratings = (ratings - np.mean(ratings)) / np.std(ratings)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ratings.flatten())\n",
    "plt.show()\n",
    "\n",
    "print(\"Loaded dataset: {}\".format(ratings.shape))\n",
    "\n",
    "n_users, n_items = ratings.shape\n",
    "print(\"ratings: max {0} - min {1}\".format(ratings.max(), ratings.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  150\n",
      "RMSE: 0.2765351286203227\n",
      "MAX_ERR: 2.876380863604467\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "\n",
    "K = 150\n",
    "U, s, Vt = svds(ratings, k=K)\n",
    "s = np.diag(s)\n",
    "U = np.dot(U, s)\n",
    "\n",
    "# MSE\n",
    "Yhat = U.dot(Vt)\n",
    "rmse = np.sqrt(np.mean(np.abs(Yhat - ratings) ** 2))\n",
    "print(\"K: \", K)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAX_ERR:\", np.abs(Yhat - ratings).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(net, normalize=False):\n",
    "\n",
    "    # Build features\n",
    "    X_pred = X\n",
    "\n",
    "    hidden_layer_sizes = list(net.hidden_layer_sizes)\n",
    "\n",
    "    layer_units = [X_pred.shape[1]] + hidden_layer_sizes + [1]\n",
    "    activations = [X_pred]\n",
    "    for i in range(net.n_layers_ - 1):\n",
    "        activations.append(np.empty((X_pred.shape[0], layer_units[i + 1])))\n",
    "\n",
    "    net._forward_pass(activations)\n",
    "    y_pred = activations[-1]\n",
    "    print(\"MSE (original):\", np.mean((y_pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get weights\n",
    "    last_w = net.coefs_[-1]\n",
    "    bias = np.array(net.intercepts_[-1]).reshape((1, 1))\n",
    "    last_w = np.concatenate([last_w, bias])\n",
    "\n",
    "    # get last-layer features\n",
    "    last_feat = np.array(activations[-2], dtype=np.float32)\n",
    "    last_feat = np.concatenate([last_feat, np.ones((X_pred.shape[0], 1))], axis=1)\n",
    "\n",
    "    # get prediction\n",
    "    pred = last_feat.dot(last_w)\n",
    "    print(\"MSE (recomputed with last layer only):\", np.mean((pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get feature matrix\n",
    "    d = hidden_layer_sizes[-1] + 1\n",
    "    print(\"d={0}\".format(d))\n",
    "    phi = np.empty((n_users, n_items, d), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for t in range(n_users):\n",
    "        for z in range(n_items):\n",
    "            phi[t, z, :] = last_feat[idx, :] / (np.linalg.norm(last_feat[idx, :]) if normalize else 1)\n",
    "            idx += 1\n",
    "    assert idx == last_feat.shape[0]\n",
    "\n",
    "    # get param\n",
    "    theta = np.array(last_w, dtype=np.float32).squeeze()\n",
    "    if normalize:\n",
    "        theta = theta / np.linalg.norm(theta)\n",
    "        \n",
    "    phi_norm = round(np.linalg.norm(phi, axis=2).max(), 2)\n",
    "    print(\"phi max norm:\", phi_norm)\n",
    "    theta_norm = round(np.linalg.norm(theta), 2)\n",
    "    print(\"theta norm:\", theta_norm)\n",
    "\n",
    "    # check predictions\n",
    "    mu = phi.dot(theta)\n",
    "    print(\"MSE (mu):\", np.mean(np.abs(ratings - mu).flatten()**2))\n",
    "    print(\"mu: max {0} - min {1}\".format(mu.max(), mu.min()))\n",
    "    gap = np.max(mu, axis=1)[:, np.newaxis] - mu\n",
    "    print(\"gap max:\", gap.max())\n",
    "    gap[gap == 0] = 100\n",
    "    print(\"gap min:\", gap.min())\n",
    "    gap = np.min(gap, axis=1)\n",
    "    print(\"# contexts with gap_min > 0.001:\", np.sum(gap > 0.001))\n",
    "    print(\"# contexts with gap_min > 0.01:\", np.sum(gap > 0.01))\n",
    "    print(\"# contexts with gap_min > 0.1:\", np.sum(gap > 0.1))\n",
    "\n",
    "    # check span\n",
    "    astar = np.argmax(mu, axis=1)\n",
    "    fstar = np.array([phi[x, astar[x]] for x in range(n_users)])\n",
    "\n",
    "    span = d\n",
    "    for i in range(d):\n",
    "        if check_spanrd(fstar, d - i):\n",
    "            span = d - i\n",
    "            break\n",
    "\n",
    "    print(\"{0}Spanning R^{1}\".format(\"WARNING: \" if span == d else \"\", span))\n",
    "    \n",
    "    # compute lambda HLS\n",
    "    \n",
    "    outer = np.matmul(fstar.T, fstar) / n_users\n",
    "    lambda_hls = np.linalg.eigvals(outer).min()\n",
    "    print(\"lambda HLS:\", lambda_hls)\n",
    "\n",
    "    # save\n",
    "#     np.savez_compressed('lastfm_d{0}_span{1}_L{2:.2f}_S{3:.2f}_hls{4:.5f}.npz'.format(d,span,phi_norm,theta_norm, lambda_hls), \n",
    "#                         features=phi, theta=theta)\n",
    "    np.savez_compressed('lastfm_d{0}_span{1}.npz'.format(d,span), features=phi, theta=theta)\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets\n",
    "\n",
    "X, y = [], []\n",
    "for t in range(n_users):\n",
    "    for z in range(n_items):\n",
    "        feat = np.concatenate([U[t], Vt[:, z]]).ravel()\n",
    "        X.append(feat)\n",
    "        y.append(ratings[t, z])\n",
    "X = np.array(X)\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [83, 38]\n",
      "Iteration 1, loss = 0.43268726\n",
      "Iteration 2, loss = 0.32261485\n",
      "Iteration 3, loss = 0.26908904\n",
      "Iteration 4, loss = 0.23441425\n",
      "Iteration 5, loss = 0.20864923\n",
      "Iteration 6, loss = 0.18765308\n",
      "Iteration 7, loss = 0.16980527\n",
      "Iteration 8, loss = 0.15485389\n",
      "Iteration 9, loss = 0.14320669\n",
      "Iteration 10, loss = 0.13371424\n",
      "Iteration 11, loss = 0.12478202\n",
      "Iteration 12, loss = 0.11696978\n",
      "Iteration 13, loss = 0.11099414\n",
      "Iteration 14, loss = 0.10556416\n",
      "Iteration 15, loss = 0.10114404\n",
      "Iteration 16, loss = 0.09666004\n",
      "Iteration 17, loss = 0.09296340\n",
      "Iteration 18, loss = 0.08911650\n",
      "Iteration 19, loss = 0.08640322\n",
      "Iteration 20, loss = 0.08318930\n",
      "Iteration 21, loss = 0.07949193\n",
      "Iteration 22, loss = 0.07704253\n",
      "Iteration 23, loss = 0.07417611\n",
      "Iteration 24, loss = 0.07167044\n",
      "Iteration 25, loss = 0.07021562\n",
      "Iteration 26, loss = 0.06793146\n",
      "Iteration 27, loss = 0.06571960\n",
      "Iteration 28, loss = 0.06479504\n",
      "Iteration 29, loss = 0.06243929\n",
      "Iteration 30, loss = 0.06083238\n",
      "Iteration 31, loss = 0.05966648\n",
      "Iteration 32, loss = 0.05865141\n",
      "Iteration 33, loss = 0.05707163\n",
      "Iteration 34, loss = 0.05572404\n",
      "Iteration 35, loss = 0.05459148\n",
      "Iteration 36, loss = 0.05246895\n",
      "Iteration 37, loss = 0.05184585\n",
      "Iteration 38, loss = 0.05104126\n",
      "Iteration 39, loss = 0.04997664\n",
      "Iteration 40, loss = 0.04857599\n",
      "Iteration 41, loss = 0.04814451\n",
      "Iteration 42, loss = 0.04640398\n",
      "Iteration 43, loss = 0.04620733\n",
      "Iteration 44, loss = 0.04585656\n",
      "Iteration 45, loss = 0.04475945\n",
      "Iteration 46, loss = 0.04387063\n",
      "Iteration 47, loss = 0.04360519\n",
      "Iteration 48, loss = 0.04284079\n",
      "Iteration 49, loss = 0.04196931\n",
      "Iteration 50, loss = 0.04140205\n",
      "Iteration 51, loss = 0.03990709\n",
      "Iteration 52, loss = 0.04043763\n",
      "Iteration 53, loss = 0.03942637\n",
      "Iteration 54, loss = 0.03999221\n",
      "Iteration 55, loss = 0.03873446\n",
      "Iteration 56, loss = 0.03796120\n",
      "Iteration 57, loss = 0.03764530\n",
      "Iteration 58, loss = 0.03773768\n",
      "Iteration 59, loss = 0.03688046\n",
      "Iteration 60, loss = 0.03603582\n",
      "Iteration 61, loss = 0.03576779\n",
      "Iteration 62, loss = 0.03526486\n",
      "Iteration 63, loss = 0.03531502\n",
      "Iteration 64, loss = 0.03488419\n",
      "Iteration 65, loss = 0.03412947\n",
      "Iteration 66, loss = 0.03401602\n",
      "Iteration 67, loss = 0.03289626\n",
      "Iteration 68, loss = 0.03342933\n",
      "Iteration 69, loss = 0.03311947\n",
      "Iteration 70, loss = 0.03256034\n",
      "Iteration 71, loss = 0.03216046\n",
      "Iteration 72, loss = 0.03187452\n",
      "Iteration 73, loss = 0.03149211\n",
      "Iteration 74, loss = 0.03109589\n",
      "Iteration 75, loss = 0.03116690\n",
      "Iteration 76, loss = 0.03004836\n",
      "Iteration 77, loss = 0.03029600\n",
      "Iteration 78, loss = 0.03032350\n",
      "Iteration 79, loss = 0.03001811\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 38): 0.6678819730926016\n",
      "Best so so far: d=38 with R^2: 0.6678819730926016\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.1236721696487414\n",
      "MSE (recomputed with last layer only): 0.12367216947028264\n",
      "d=39\n",
      "phi max norm: 34.59\n",
      "theta norm: 2.2\n",
      "MSE (mu): 0.1236721685670775\n",
      "mu: max 8.694480895996094 - min -1.1845316886901855\n",
      "gap max: 9.372555\n",
      "gap min: 0.0003004074\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1285\n",
      "# contexts with gap_min > 0.1: 1042\n",
      "WARNING: Spanning R^39\n",
      "lambda HLS: 0.020839013\n",
      "\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [70, 70, 39]\n",
      "Iteration 1, loss = 0.42364068\n",
      "Iteration 2, loss = 0.31702781\n",
      "Iteration 3, loss = 0.25447167\n",
      "Iteration 4, loss = 0.21288953\n",
      "Iteration 5, loss = 0.18405276\n",
      "Iteration 6, loss = 0.16338507\n",
      "Iteration 7, loss = 0.14683832\n",
      "Iteration 8, loss = 0.13409347\n",
      "Iteration 9, loss = 0.12484156\n",
      "Iteration 10, loss = 0.11679639\n",
      "Iteration 11, loss = 0.11042370\n",
      "Iteration 12, loss = 0.10465591\n",
      "Iteration 13, loss = 0.09802637\n",
      "Iteration 14, loss = 0.09592799\n",
      "Iteration 15, loss = 0.09074560\n",
      "Iteration 16, loss = 0.08701027\n",
      "Iteration 17, loss = 0.08329105\n",
      "Iteration 18, loss = 0.08130939\n",
      "Iteration 19, loss = 0.07915236\n",
      "Iteration 20, loss = 0.07615053\n",
      "Iteration 21, loss = 0.07430894\n",
      "Iteration 22, loss = 0.07306819\n",
      "Iteration 23, loss = 0.07077611\n",
      "Iteration 24, loss = 0.06955673\n",
      "Iteration 25, loss = 0.06831187\n",
      "Iteration 26, loss = 0.06610881\n",
      "Iteration 27, loss = 0.06423299\n",
      "Iteration 28, loss = 0.06409778\n",
      "Iteration 29, loss = 0.06369686\n",
      "Iteration 30, loss = 0.06233986\n",
      "Iteration 31, loss = 0.06024932\n",
      "Iteration 32, loss = 0.05949785\n",
      "Iteration 33, loss = 0.05867322\n",
      "Iteration 34, loss = 0.05766580\n",
      "Iteration 35, loss = 0.05736516\n",
      "Iteration 36, loss = 0.05608216\n",
      "Iteration 37, loss = 0.05583994\n",
      "Iteration 38, loss = 0.05554213\n",
      "Iteration 39, loss = 0.05408748\n",
      "Iteration 40, loss = 0.05415237\n",
      "Iteration 41, loss = 0.05425245\n",
      "Iteration 42, loss = 0.05252040\n",
      "Iteration 43, loss = 0.05087064\n",
      "Iteration 44, loss = 0.05149141\n",
      "Iteration 45, loss = 0.05068521\n",
      "Iteration 46, loss = 0.05004171\n",
      "Iteration 47, loss = 0.04891474\n",
      "Iteration 48, loss = 0.04966147\n",
      "Iteration 49, loss = 0.04906049\n",
      "Iteration 50, loss = 0.04775648\n",
      "Iteration 51, loss = 0.04733270\n",
      "Iteration 52, loss = 0.04687528\n",
      "Iteration 53, loss = 0.04572379\n",
      "Iteration 54, loss = 0.04563815\n",
      "Iteration 55, loss = 0.04576900\n",
      "Iteration 56, loss = 0.04556423\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 39): 0.5740638640822189\n",
      "Best so so far: d=38 with R^2: 0.6678819730926016\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.167572449952512\n",
      "MSE (recomputed with last layer only): 0.16757245000004695\n",
      "d=40\n",
      "phi max norm: 56.14\n",
      "theta norm: 1.99\n",
      "MSE (mu): 0.16757245314285368\n",
      "mu: max 6.528672218322754 - min -0.7519567012786865\n",
      "gap max: 6.872711\n",
      "gap min: 0.0001654625\n",
      "# contexts with gap_min > 0.001: 1316\n",
      "# contexts with gap_min > 0.01: 1265\n",
      "# contexts with gap_min > 0.1: 970\n",
      "WARNING: Spanning R^40\n",
      "lambda HLS: 0.0028426219\n",
      "\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [131, 131, 23]\n",
      "Iteration 1, loss = 0.41087277\n",
      "Iteration 2, loss = 0.27578828\n",
      "Iteration 3, loss = 0.19361081\n",
      "Iteration 4, loss = 0.14382972\n",
      "Iteration 5, loss = 0.11179128\n",
      "Iteration 6, loss = 0.09159375\n",
      "Iteration 7, loss = 0.07983717\n",
      "Iteration 8, loss = 0.06884117\n",
      "Iteration 9, loss = 0.06374059\n",
      "Iteration 10, loss = 0.05827930\n",
      "Iteration 11, loss = 0.05385186\n",
      "Iteration 12, loss = 0.05150276\n",
      "Iteration 13, loss = 0.04913658\n",
      "Iteration 14, loss = 0.04634399\n",
      "Iteration 15, loss = 0.04350778\n",
      "Iteration 16, loss = 0.04218934\n",
      "Iteration 17, loss = 0.04049932\n",
      "Iteration 18, loss = 0.03822455\n",
      "Iteration 19, loss = 0.03701479\n",
      "Iteration 20, loss = 0.03632169\n",
      "Iteration 21, loss = 0.03540727\n",
      "Iteration 22, loss = 0.03236516\n",
      "Iteration 23, loss = 0.03502852\n",
      "Iteration 24, loss = 0.03290962\n",
      "Iteration 25, loss = 0.02941331\n",
      "Iteration 26, loss = 0.02985320\n",
      "Iteration 27, loss = 0.03047842\n",
      "Iteration 28, loss = 0.02988373\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 23): 0.7090358322700632\n",
      "Best so so far: d=23 with R^2: 0.7090358322700632\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.11597057518072137\n",
      "MSE (recomputed with last layer only): 0.11597057516680728\n",
      "d=24\n",
      "phi max norm: 42.38\n",
      "theta norm: 1.25\n",
      "MSE (mu): 0.11597057642786096\n",
      "mu: max 5.909378528594971 - min -0.6319982409477234\n",
      "gap max: 6.312868\n",
      "gap min: 0.00071549416\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1285\n",
      "# contexts with gap_min > 0.1: 966\n",
      "Spanning R^21\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [152, 9]\n",
      "Iteration 1, loss = 0.43364539\n",
      "Iteration 2, loss = 0.28863690\n",
      "Iteration 3, loss = 0.18402220\n",
      "Iteration 4, loss = 0.12201542\n",
      "Iteration 5, loss = 0.09153358\n",
      "Iteration 6, loss = 0.07340509\n",
      "Iteration 7, loss = 0.06120796\n",
      "Iteration 8, loss = 0.05233589\n",
      "Iteration 9, loss = 0.04625724\n",
      "Iteration 10, loss = 0.04262267\n",
      "Iteration 11, loss = 0.03859253\n",
      "Iteration 12, loss = 0.03738560\n",
      "Iteration 13, loss = 0.03359002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.03334356\n",
      "Iteration 15, loss = 0.03201895\n",
      "Iteration 16, loss = 0.02841670\n",
      "Iteration 17, loss = 0.02881338\n",
      "Iteration 18, loss = 0.02686945\n",
      "Iteration 19, loss = 0.02668306\n",
      "Iteration 20, loss = 0.02492584\n",
      "Iteration 21, loss = 0.02573857\n",
      "Iteration 22, loss = 0.02418994\n",
      "Iteration 23, loss = 0.02353164\n",
      "Iteration 24, loss = 0.02291382\n",
      "Iteration 25, loss = 0.02321300\n",
      "Iteration 26, loss = 0.02159530\n",
      "Iteration 27, loss = 0.02119009\n",
      "Iteration 28, loss = 0.01943185\n",
      "Iteration 29, loss = 0.02027242\n",
      "Iteration 30, loss = 0.02196137\n",
      "Iteration 31, loss = 0.01842263\n",
      "Iteration 32, loss = 0.01913085\n",
      "Iteration 33, loss = 0.01774196\n",
      "Iteration 34, loss = 0.01987111\n",
      "Iteration 35, loss = 0.01774628\n",
      "Iteration 36, loss = 0.01640196\n",
      "Iteration 37, loss = 0.01704667\n",
      "Iteration 38, loss = 0.01549132\n",
      "Iteration 39, loss = 0.01574106\n",
      "Iteration 40, loss = 0.01690628\n",
      "Iteration 41, loss = 0.01462669\n",
      "Iteration 42, loss = 0.01611594\n",
      "Iteration 43, loss = 0.01592318\n",
      "Iteration 44, loss = 0.01361727\n",
      "Iteration 45, loss = 0.01522809\n",
      "Iteration 46, loss = 0.01272073\n",
      "Iteration 47, loss = 0.01492910\n",
      "Iteration 48, loss = 0.01349846\n",
      "Iteration 49, loss = 0.01444106\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 9): 0.8316372687795281\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06375650512008485\n",
      "MSE (recomputed with last layer only): 0.06375650511221972\n",
      "d=10\n",
      "phi max norm: 28.22\n",
      "theta norm: 1.3\n",
      "MSE (mu): 0.06375651328878171\n",
      "mu: max 6.3255720138549805 - min -0.7405591011047363\n",
      "gap max: 6.6486993\n",
      "gap min: 0.00025177002\n",
      "# contexts with gap_min > 0.001: 1318\n",
      "# contexts with gap_min > 0.01: 1271\n",
      "# contexts with gap_min > 0.1: 984\n",
      "WARNING: Spanning R^10\n",
      "lambda HLS: 0.018261189\n",
      "\n",
      "[5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [115, 115, 45]\n",
      "Iteration 1, loss = 0.40395869\n",
      "Iteration 2, loss = 0.28349629\n",
      "Iteration 3, loss = 0.20643685\n",
      "Iteration 4, loss = 0.16047102\n",
      "Iteration 5, loss = 0.13023930\n",
      "Iteration 6, loss = 0.10851497\n",
      "Iteration 7, loss = 0.09302539\n",
      "Iteration 8, loss = 0.08232467\n",
      "Iteration 9, loss = 0.07311371\n",
      "Iteration 10, loss = 0.06816389\n",
      "Iteration 11, loss = 0.06173454\n",
      "Iteration 12, loss = 0.05851712\n",
      "Iteration 13, loss = 0.05433100\n",
      "Iteration 14, loss = 0.05144710\n",
      "Iteration 15, loss = 0.04884473\n",
      "Iteration 16, loss = 0.04742229\n",
      "Iteration 17, loss = 0.04536032\n",
      "Iteration 18, loss = 0.04319793\n",
      "Iteration 19, loss = 0.04190742\n",
      "Iteration 20, loss = 0.03939041\n",
      "Iteration 21, loss = 0.03898792\n",
      "Iteration 22, loss = 0.03797231\n",
      "Iteration 23, loss = 0.03712984\n",
      "Iteration 24, loss = 0.03597479\n",
      "Iteration 25, loss = 0.03465025\n",
      "Iteration 26, loss = 0.03401704\n",
      "Iteration 27, loss = 0.03211137\n",
      "Iteration 28, loss = 0.03342779\n",
      "Iteration 29, loss = 0.03128112\n",
      "Iteration 30, loss = 0.03051695\n",
      "Iteration 31, loss = 0.03190883\n",
      "Iteration 32, loss = 0.02865449\n",
      "Iteration 33, loss = 0.02943305\n",
      "Iteration 34, loss = 0.02863205\n",
      "Iteration 35, loss = 0.02907741\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 45): 0.7047372097887896\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.1152007463913232\n",
      "MSE (recomputed with last layer only): 0.11520074615889123\n",
      "d=46\n",
      "phi max norm: 55.28\n",
      "theta norm: 1.46\n",
      "MSE (mu): 0.11520074617848686\n",
      "mu: max 6.38197135925293 - min -0.6120615601539612\n",
      "gap max: 6.726304\n",
      "gap min: 0.00016593933\n",
      "# contexts with gap_min > 0.001: 1319\n",
      "# contexts with gap_min > 0.01: 1272\n",
      "# contexts with gap_min > 0.1: 960\n",
      "Spanning R^43\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [88, 13]\n",
      "Iteration 1, loss = 0.45434633\n",
      "Iteration 2, loss = 0.32794932\n",
      "Iteration 3, loss = 0.26385995\n",
      "Iteration 4, loss = 0.21980564\n",
      "Iteration 5, loss = 0.18537883\n",
      "Iteration 6, loss = 0.15956749\n",
      "Iteration 7, loss = 0.14300246\n",
      "Iteration 8, loss = 0.12992053\n",
      "Iteration 9, loss = 0.12066671\n",
      "Iteration 10, loss = 0.11179900\n",
      "Iteration 11, loss = 0.10540610\n",
      "Iteration 12, loss = 0.09899124\n",
      "Iteration 13, loss = 0.09426775\n",
      "Iteration 14, loss = 0.09026988\n",
      "Iteration 15, loss = 0.08659504\n",
      "Iteration 16, loss = 0.08346209\n",
      "Iteration 17, loss = 0.08079373\n",
      "Iteration 18, loss = 0.07722921\n",
      "Iteration 19, loss = 0.07408064\n",
      "Iteration 20, loss = 0.07144557\n",
      "Iteration 21, loss = 0.06915130\n",
      "Iteration 22, loss = 0.06628634\n",
      "Iteration 23, loss = 0.06503343\n",
      "Iteration 24, loss = 0.06310762\n",
      "Iteration 25, loss = 0.06132930\n",
      "Iteration 26, loss = 0.05977387\n",
      "Iteration 27, loss = 0.05843873\n",
      "Iteration 28, loss = 0.05738566\n",
      "Iteration 29, loss = 0.05593251\n",
      "Iteration 30, loss = 0.05486737\n",
      "Iteration 31, loss = 0.05328327\n",
      "Iteration 32, loss = 0.05238703\n",
      "Iteration 33, loss = 0.05173725\n",
      "Iteration 34, loss = 0.05037000\n",
      "Iteration 35, loss = 0.05059391\n",
      "Iteration 36, loss = 0.04882182\n",
      "Iteration 37, loss = 0.04787318\n",
      "Iteration 38, loss = 0.04707910\n",
      "Iteration 39, loss = 0.04656717\n",
      "Iteration 40, loss = 0.04576206\n",
      "Iteration 41, loss = 0.04461539\n",
      "Iteration 42, loss = 0.04526684\n",
      "Iteration 43, loss = 0.04361999\n",
      "Iteration 44, loss = 0.04315769\n",
      "Iteration 45, loss = 0.04277932\n",
      "Iteration 46, loss = 0.04214662\n",
      "Iteration 47, loss = 0.04139825\n",
      "Iteration 48, loss = 0.04107694\n",
      "Iteration 49, loss = 0.04056009\n",
      "Iteration 50, loss = 0.03977933\n",
      "Iteration 51, loss = 0.03987092\n",
      "Iteration 52, loss = 0.03870366\n",
      "Iteration 53, loss = 0.03851261\n",
      "Iteration 54, loss = 0.03775828\n",
      "Iteration 55, loss = 0.03810857\n",
      "Iteration 56, loss = 0.03674702\n",
      "Iteration 57, loss = 0.03676459\n",
      "Iteration 58, loss = 0.03654513\n",
      "Iteration 59, loss = 0.03642696\n",
      "Iteration 60, loss = 0.03539307\n",
      "Iteration 61, loss = 0.03481948\n",
      "Iteration 62, loss = 0.03511379\n",
      "Iteration 63, loss = 0.03483766\n",
      "Iteration 64, loss = 0.03332319\n",
      "Iteration 65, loss = 0.03343938\n",
      "Iteration 66, loss = 0.03340958\n",
      "Iteration 67, loss = 0.03294826\n",
      "Iteration 68, loss = 0.03285556\n",
      "Iteration 69, loss = 0.03198625\n",
      "Iteration 70, loss = 0.03179868\n",
      "Iteration 71, loss = 0.03174822\n",
      "Iteration 72, loss = 0.03100995\n",
      "Iteration 73, loss = 0.03094625\n",
      "Iteration 74, loss = 0.03125186\n",
      "Iteration 75, loss = 0.03022817\n",
      "Iteration 76, loss = 0.03005836\n",
      "Iteration 77, loss = 0.02989055\n",
      "Iteration 78, loss = 0.02970861\n",
      "Iteration 79, loss = 0.02955556\n",
      "Iteration 80, loss = 0.02964617\n",
      "Iteration 81, loss = 0.02885026\n",
      "Iteration 82, loss = 0.02893006\n",
      "Iteration 83, loss = 0.02874470\n",
      "Iteration 84, loss = 0.02848203\n",
      "Iteration 85, loss = 0.02865102\n",
      "Iteration 86, loss = 0.02849818\n",
      "Iteration 87, loss = 0.02789479\n",
      "Iteration 88, loss = 0.02744048\n",
      "Iteration 89, loss = 0.02773378\n",
      "Iteration 90, loss = 0.02764990\n",
      "Iteration 91, loss = 0.02751620\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 13): 0.6638668112906158\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.12197447847308318\n",
      "MSE (recomputed with last layer only): 0.12197447853145087\n",
      "d=14\n",
      "phi max norm: 30.26\n",
      "theta norm: 1.81\n",
      "MSE (mu): 0.1219744802679196\n",
      "mu: max 10.202235221862793 - min -0.6647334098815918\n",
      "gap max: 10.546464\n",
      "gap min: 0.00027647614\n",
      "# contexts with gap_min > 0.001: 1319\n",
      "# contexts with gap_min > 0.01: 1299\n",
      "# contexts with gap_min > 0.1: 1095\n",
      "WARNING: Spanning R^14\n",
      "lambda HLS: 0.005630109\n",
      "\n",
      "[5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [95, 95, 19]\n",
      "Iteration 1, loss = 0.42581939\n",
      "Iteration 2, loss = 0.30624363\n",
      "Iteration 3, loss = 0.23447165\n",
      "Iteration 4, loss = 0.18687034\n",
      "Iteration 5, loss = 0.15605945\n",
      "Iteration 6, loss = 0.13482770\n",
      "Iteration 7, loss = 0.11777979\n",
      "Iteration 8, loss = 0.10485288\n",
      "Iteration 9, loss = 0.09358852\n",
      "Iteration 10, loss = 0.08424439\n",
      "Iteration 11, loss = 0.07801847\n",
      "Iteration 12, loss = 0.07316098\n",
      "Iteration 13, loss = 0.06828304\n",
      "Iteration 14, loss = 0.06444509\n",
      "Iteration 15, loss = 0.06133584\n",
      "Iteration 16, loss = 0.05803520\n",
      "Iteration 17, loss = 0.05610425\n",
      "Iteration 18, loss = 0.05477557\n",
      "Iteration 19, loss = 0.05153572\n",
      "Iteration 20, loss = 0.05159731\n",
      "Iteration 21, loss = 0.04815069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.04737184\n",
      "Iteration 23, loss = 0.04677774\n",
      "Iteration 24, loss = 0.04436162\n",
      "Iteration 25, loss = 0.04320165\n",
      "Iteration 26, loss = 0.04311047\n",
      "Iteration 27, loss = 0.04313566\n",
      "Iteration 28, loss = 0.04040533\n",
      "Iteration 29, loss = 0.03974548\n",
      "Iteration 30, loss = 0.03836733\n",
      "Iteration 31, loss = 0.03838724\n",
      "Iteration 32, loss = 0.03832966\n",
      "Iteration 33, loss = 0.03722778\n",
      "Iteration 34, loss = 0.03514612\n",
      "Iteration 35, loss = 0.03549898\n",
      "Iteration 36, loss = 0.03517404\n",
      "Iteration 37, loss = 0.03374588\n",
      "Iteration 38, loss = 0.03354777\n",
      "Iteration 39, loss = 0.03310621\n",
      "Iteration 40, loss = 0.03220856\n",
      "Iteration 41, loss = 0.03288189\n",
      "Iteration 42, loss = 0.03178171\n",
      "Iteration 43, loss = 0.03104920\n",
      "Iteration 44, loss = 0.03079451\n",
      "Iteration 45, loss = 0.03038558\n",
      "Iteration 46, loss = 0.02871424\n",
      "Iteration 47, loss = 0.03069981\n",
      "Iteration 48, loss = 0.02967848\n",
      "Iteration 49, loss = 0.02763391\n",
      "Iteration 50, loss = 0.02828420\n",
      "Iteration 51, loss = 0.02793522\n",
      "Iteration 52, loss = 0.02911797\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 19): 0.6886568208085846\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.11123796357107814\n",
      "MSE (recomputed with last layer only): 0.11123796324218777\n",
      "d=20\n",
      "phi max norm: 69.82\n",
      "theta norm: 1.17\n",
      "MSE (mu): 0.11123796337820181\n",
      "mu: max 6.5577826499938965 - min -0.7106533646583557\n",
      "gap max: 6.9330373\n",
      "gap min: 5.698204e-05\n",
      "# contexts with gap_min > 0.001: 1318\n",
      "# contexts with gap_min > 0.01: 1280\n",
      "# contexts with gap_min > 0.1: 965\n",
      "WARNING: Spanning R^20\n",
      "lambda HLS: 0.0045695207\n",
      "\n",
      "[5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [111, 27]\n",
      "Iteration 1, loss = 0.42248429\n",
      "Iteration 2, loss = 0.31583631\n",
      "Iteration 3, loss = 0.24547495\n",
      "Iteration 4, loss = 0.19678508\n",
      "Iteration 5, loss = 0.16318131\n",
      "Iteration 6, loss = 0.13914374\n",
      "Iteration 7, loss = 0.12164494\n",
      "Iteration 8, loss = 0.10878648\n",
      "Iteration 9, loss = 0.09983684\n",
      "Iteration 10, loss = 0.09105952\n",
      "Iteration 11, loss = 0.08446593\n",
      "Iteration 12, loss = 0.07883424\n",
      "Iteration 13, loss = 0.07381116\n",
      "Iteration 14, loss = 0.06990007\n",
      "Iteration 15, loss = 0.06576640\n",
      "Iteration 16, loss = 0.06188404\n",
      "Iteration 17, loss = 0.05877975\n",
      "Iteration 18, loss = 0.05670960\n",
      "Iteration 19, loss = 0.05372064\n",
      "Iteration 20, loss = 0.05108230\n",
      "Iteration 21, loss = 0.04888720\n",
      "Iteration 22, loss = 0.04681480\n",
      "Iteration 23, loss = 0.04569491\n",
      "Iteration 24, loss = 0.04322799\n",
      "Iteration 25, loss = 0.04163779\n",
      "Iteration 26, loss = 0.04084476\n",
      "Iteration 27, loss = 0.03936893\n",
      "Iteration 28, loss = 0.03827344\n",
      "Iteration 29, loss = 0.03716458\n",
      "Iteration 30, loss = 0.03567443\n",
      "Iteration 31, loss = 0.03506085\n",
      "Iteration 32, loss = 0.03372496\n",
      "Iteration 33, loss = 0.03400784\n",
      "Iteration 34, loss = 0.03287210\n",
      "Iteration 35, loss = 0.03208521\n",
      "Iteration 36, loss = 0.03099232\n",
      "Iteration 37, loss = 0.02995305\n",
      "Iteration 38, loss = 0.02945659\n",
      "Iteration 39, loss = 0.02922916\n",
      "Iteration 40, loss = 0.02888957\n",
      "Iteration 41, loss = 0.02828516\n",
      "Iteration 42, loss = 0.02781504\n",
      "Iteration 43, loss = 0.02650938\n",
      "Iteration 44, loss = 0.02739470\n",
      "Iteration 45, loss = 0.02697023\n",
      "Iteration 46, loss = 0.02583963\n",
      "Iteration 47, loss = 0.02563569\n",
      "Iteration 48, loss = 0.02529260\n",
      "Iteration 49, loss = 0.02485166\n",
      "Iteration 50, loss = 0.02451101\n",
      "Iteration 51, loss = 0.02385480\n",
      "Iteration 52, loss = 0.02309798\n",
      "Iteration 53, loss = 0.02337537\n",
      "Iteration 54, loss = 0.02333553\n",
      "Iteration 55, loss = 0.02227244\n",
      "Iteration 56, loss = 0.02243401\n",
      "Iteration 57, loss = 0.02216808\n",
      "Iteration 58, loss = 0.02158477\n",
      "Iteration 59, loss = 0.02138126\n",
      "Iteration 60, loss = 0.02107606\n",
      "Iteration 61, loss = 0.02077286\n",
      "Iteration 62, loss = 0.02180968\n",
      "Iteration 63, loss = 0.02047243\n",
      "Iteration 64, loss = 0.02000165\n",
      "Iteration 65, loss = 0.02022167\n",
      "Iteration 66, loss = 0.01968962\n",
      "Iteration 67, loss = 0.01982472\n",
      "Iteration 68, loss = 0.02024411\n",
      "Iteration 69, loss = 0.01922146\n",
      "Iteration 70, loss = 0.01870758\n",
      "Iteration 71, loss = 0.01951143\n",
      "Iteration 72, loss = 0.01852120\n",
      "Iteration 73, loss = 0.01844674\n",
      "Iteration 74, loss = 0.01885683\n",
      "Iteration 75, loss = 0.01846804\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 27): 0.7339026196739098\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.09398516286691183\n",
      "MSE (recomputed with last layer only): 0.09398516283247389\n",
      "d=28\n",
      "phi max norm: 32.15\n",
      "theta norm: 1.99\n",
      "MSE (mu): 0.09398516523913159\n",
      "mu: max 10.411738395690918 - min -1.004271149635315\n",
      "gap max: 10.735121\n",
      "gap min: 0.0005514622\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1297\n",
      "# contexts with gap_min > 0.1: 1051\n",
      "Spanning R^27\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 8, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [162, 7]\n",
      "Iteration 1, loss = 0.42601783\n",
      "Iteration 2, loss = 0.31323926\n",
      "Iteration 3, loss = 0.23392112\n",
      "Iteration 4, loss = 0.17146865\n",
      "Iteration 5, loss = 0.12525587\n",
      "Iteration 6, loss = 0.09624997\n",
      "Iteration 7, loss = 0.07959463\n",
      "Iteration 8, loss = 0.06740762\n",
      "Iteration 9, loss = 0.05985160\n",
      "Iteration 10, loss = 0.05258923\n",
      "Iteration 11, loss = 0.04899255\n",
      "Iteration 12, loss = 0.04444159\n",
      "Iteration 13, loss = 0.04199399\n",
      "Iteration 14, loss = 0.03902722\n",
      "Iteration 15, loss = 0.03599982\n",
      "Iteration 16, loss = 0.03553681\n",
      "Iteration 17, loss = 0.03252309\n",
      "Iteration 18, loss = 0.03146395\n",
      "Iteration 19, loss = 0.03001890\n",
      "Iteration 20, loss = 0.02820840\n",
      "Iteration 21, loss = 0.02738659\n",
      "Iteration 22, loss = 0.02666862\n",
      "Iteration 23, loss = 0.02534781\n",
      "Iteration 24, loss = 0.02476367\n",
      "Iteration 25, loss = 0.02314117\n",
      "Iteration 26, loss = 0.02288408\n",
      "Iteration 27, loss = 0.02210723\n",
      "Iteration 28, loss = 0.02126779\n",
      "Iteration 29, loss = 0.02067204\n",
      "Iteration 30, loss = 0.02071368\n",
      "Iteration 31, loss = 0.01991249\n",
      "Iteration 32, loss = 0.01913092\n",
      "Iteration 33, loss = 0.01868650\n",
      "Iteration 34, loss = 0.01846446\n",
      "Iteration 35, loss = 0.01778430\n",
      "Iteration 36, loss = 0.01720722\n",
      "Iteration 37, loss = 0.01706584\n",
      "Iteration 38, loss = 0.01669627\n",
      "Iteration 39, loss = 0.01678629\n",
      "Iteration 40, loss = 0.01618052\n",
      "Iteration 41, loss = 0.01552606\n",
      "Iteration 42, loss = 0.01515584\n",
      "Iteration 43, loss = 0.01525978\n",
      "Iteration 44, loss = 0.01484027\n",
      "Iteration 45, loss = 0.01441958\n",
      "Iteration 46, loss = 0.01466037\n",
      "Iteration 47, loss = 0.01376805\n",
      "Iteration 48, loss = 0.01352765\n",
      "Iteration 49, loss = 0.01374453\n",
      "Iteration 50, loss = 0.01331662\n",
      "Iteration 51, loss = 0.01361675\n",
      "Iteration 52, loss = 0.01286763\n",
      "Iteration 53, loss = 0.01267457\n",
      "Iteration 54, loss = 0.01246359\n",
      "Iteration 55, loss = 0.01254752\n",
      "Iteration 56, loss = 0.01193014\n",
      "Iteration 57, loss = 0.01222199\n",
      "Iteration 58, loss = 0.01218705\n",
      "Iteration 59, loss = 0.01183297\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 7): 0.8235948751214184\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.060479278538259615\n",
      "MSE (recomputed with last layer only): 0.06047927856530464\n",
      "d=8\n",
      "phi max norm: 20.93\n",
      "theta norm: 0.87\n",
      "MSE (mu): 0.06047927640961043\n",
      "mu: max 7.829695701599121 - min -0.3399736285209656\n",
      "gap max: 8.160714\n",
      "gap min: 0.00012540817\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1302\n",
      "# contexts with gap_min > 0.1: 1109\n",
      "Spanning R^6\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 8, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [91, 43]\n",
      "Iteration 1, loss = 0.42697898\n",
      "Iteration 2, loss = 0.31251763\n",
      "Iteration 3, loss = 0.25497957\n",
      "Iteration 4, loss = 0.21721728\n",
      "Iteration 5, loss = 0.19032890\n",
      "Iteration 6, loss = 0.16954120\n",
      "Iteration 7, loss = 0.15233948\n",
      "Iteration 8, loss = 0.13894490\n",
      "Iteration 9, loss = 0.12794526\n",
      "Iteration 10, loss = 0.11850607\n",
      "Iteration 11, loss = 0.11068621\n",
      "Iteration 12, loss = 0.10388767\n",
      "Iteration 13, loss = 0.09874762\n",
      "Iteration 14, loss = 0.09407407\n",
      "Iteration 15, loss = 0.09071247\n",
      "Iteration 16, loss = 0.08625180\n",
      "Iteration 17, loss = 0.08204790\n",
      "Iteration 18, loss = 0.07950646\n",
      "Iteration 19, loss = 0.07684810\n",
      "Iteration 20, loss = 0.07496978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.07229524\n",
      "Iteration 22, loss = 0.06985758\n",
      "Iteration 23, loss = 0.06746905\n",
      "Iteration 24, loss = 0.06520739\n",
      "Iteration 25, loss = 0.06327510\n",
      "Iteration 26, loss = 0.06127362\n",
      "Iteration 27, loss = 0.05979240\n",
      "Iteration 28, loss = 0.05793389\n",
      "Iteration 29, loss = 0.05719622\n",
      "Iteration 30, loss = 0.05545148\n",
      "Iteration 31, loss = 0.05412282\n",
      "Iteration 32, loss = 0.05263762\n",
      "Iteration 33, loss = 0.05183867\n",
      "Iteration 34, loss = 0.05087307\n",
      "Iteration 35, loss = 0.04937144\n",
      "Iteration 36, loss = 0.04831563\n",
      "Iteration 37, loss = 0.04762664\n",
      "Iteration 38, loss = 0.04604153\n",
      "Iteration 39, loss = 0.04502852\n",
      "Iteration 40, loss = 0.04460963\n",
      "Iteration 41, loss = 0.04334505\n",
      "Iteration 42, loss = 0.04299720\n",
      "Iteration 43, loss = 0.04236579\n",
      "Iteration 44, loss = 0.04130850\n",
      "Iteration 45, loss = 0.04084696\n",
      "Iteration 46, loss = 0.04040747\n",
      "Iteration 47, loss = 0.03950550\n",
      "Iteration 48, loss = 0.03935537\n",
      "Iteration 49, loss = 0.03839440\n",
      "Iteration 50, loss = 0.03737080\n",
      "Iteration 51, loss = 0.03712950\n",
      "Iteration 52, loss = 0.03685521\n",
      "Iteration 53, loss = 0.03610738\n",
      "Iteration 54, loss = 0.03625170\n",
      "Iteration 55, loss = 0.03446778\n",
      "Iteration 56, loss = 0.03461621\n",
      "Iteration 57, loss = 0.03457531\n",
      "Iteration 58, loss = 0.03463006\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 43): 0.6696633185511959\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.13290081460696174\n",
      "MSE (recomputed with last layer only): 0.13290081471804885\n",
      "d=44\n",
      "phi max norm: 25.66\n",
      "theta norm: 2.38\n",
      "MSE (mu): 0.13290081676427556\n",
      "mu: max 10.059526443481445 - min -1.1300647258758545\n",
      "gap max: 10.614958\n",
      "gap min: 6.484985e-05\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1293\n",
      "# contexts with gap_min > 0.1: 1093\n",
      "WARNING: Spanning R^44\n",
      "lambda HLS: 0.02183226\n",
      "\n",
      "[5, 6, 8, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [100, 24]\n",
      "Iteration 1, loss = 0.43114485\n",
      "Iteration 2, loss = 0.31333621\n",
      "Iteration 3, loss = 0.25367082\n",
      "Iteration 4, loss = 0.21103362\n",
      "Iteration 5, loss = 0.17999742\n",
      "Iteration 6, loss = 0.15487123\n",
      "Iteration 7, loss = 0.13498127\n",
      "Iteration 8, loss = 0.12254995\n",
      "Iteration 9, loss = 0.10885065\n",
      "Iteration 10, loss = 0.10095233\n",
      "Iteration 11, loss = 0.09368995\n",
      "Iteration 12, loss = 0.08818853\n",
      "Iteration 13, loss = 0.08263823\n",
      "Iteration 14, loss = 0.07808683\n",
      "Iteration 15, loss = 0.07492378\n",
      "Iteration 16, loss = 0.07233751\n",
      "Iteration 17, loss = 0.06784312\n",
      "Iteration 18, loss = 0.06523912\n",
      "Iteration 19, loss = 0.06293609\n",
      "Iteration 20, loss = 0.06102700\n",
      "Iteration 21, loss = 0.05854180\n",
      "Iteration 22, loss = 0.05671785\n",
      "Iteration 23, loss = 0.05542320\n",
      "Iteration 24, loss = 0.05363703\n",
      "Iteration 25, loss = 0.05227606\n",
      "Iteration 26, loss = 0.05028098\n",
      "Iteration 27, loss = 0.04919836\n",
      "Iteration 28, loss = 0.04727732\n",
      "Iteration 29, loss = 0.04797463\n",
      "Iteration 30, loss = 0.04605102\n",
      "Iteration 31, loss = 0.04457878\n",
      "Iteration 32, loss = 0.04420366\n",
      "Iteration 33, loss = 0.04188532\n",
      "Iteration 34, loss = 0.04227959\n",
      "Iteration 35, loss = 0.04074771\n",
      "Iteration 36, loss = 0.03993769\n",
      "Iteration 37, loss = 0.03842833\n",
      "Iteration 38, loss = 0.03774485\n",
      "Iteration 39, loss = 0.03632250\n",
      "Iteration 40, loss = 0.03560285\n",
      "Iteration 41, loss = 0.03494927\n",
      "Iteration 42, loss = 0.03366252\n",
      "Iteration 43, loss = 0.03293375\n",
      "Iteration 44, loss = 0.03146193\n",
      "Iteration 45, loss = 0.03072958\n",
      "Iteration 46, loss = 0.03144197\n",
      "Iteration 47, loss = 0.03006234\n",
      "Iteration 48, loss = 0.02905978\n",
      "Iteration 49, loss = 0.02951801\n",
      "Iteration 50, loss = 0.02837806\n",
      "Iteration 51, loss = 0.02842445\n",
      "Iteration 52, loss = 0.02791120\n",
      "Iteration 53, loss = 0.02703863\n",
      "Iteration 54, loss = 0.02680423\n",
      "Iteration 55, loss = 0.02749147\n",
      "Iteration 56, loss = 0.02602473\n",
      "Iteration 57, loss = 0.02523471\n",
      "Iteration 58, loss = 0.02534742\n",
      "Iteration 59, loss = 0.02517583\n",
      "Iteration 60, loss = 0.02447385\n",
      "Iteration 61, loss = 0.02466773\n",
      "Iteration 62, loss = 0.02433900\n",
      "Iteration 63, loss = 0.02438216\n",
      "Iteration 64, loss = 0.02350969\n",
      "Iteration 65, loss = 0.02327040\n",
      "Iteration 66, loss = 0.02298810\n",
      "Iteration 67, loss = 0.02300564\n",
      "Iteration 68, loss = 0.02266813\n",
      "Iteration 69, loss = 0.02188989\n",
      "Iteration 70, loss = 0.02256118\n",
      "Iteration 71, loss = 0.02176769\n",
      "Iteration 72, loss = 0.02155774\n",
      "Iteration 73, loss = 0.02131391\n",
      "Iteration 74, loss = 0.02164616\n",
      "Iteration 75, loss = 0.02051726\n",
      "Iteration 76, loss = 0.02073780\n",
      "Iteration 77, loss = 0.02015930\n",
      "Iteration 78, loss = 0.02030444\n",
      "Iteration 79, loss = 0.02033488\n",
      "Iteration 80, loss = 0.02003915\n",
      "Iteration 81, loss = 0.01995215\n",
      "Iteration 82, loss = 0.01954732\n",
      "Iteration 83, loss = 0.01968868\n",
      "Iteration 84, loss = 0.01928056\n",
      "Iteration 85, loss = 0.01906684\n",
      "Iteration 86, loss = 0.01858857\n",
      "Iteration 87, loss = 0.01914146\n",
      "Iteration 88, loss = 0.01877317\n",
      "Iteration 89, loss = 0.01889475\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 24): 0.7225930975204522\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.09690521481125668\n",
      "MSE (recomputed with last layer only): 0.0969052147656699\n",
      "d=25\n",
      "phi max norm: 32.24\n",
      "theta norm: 1.81\n",
      "MSE (mu): 0.09690521625251791\n",
      "mu: max 8.734891891479492 - min -0.9666376709938049\n",
      "gap max: 9.232751\n",
      "gap min: 0.00055623055\n",
      "# contexts with gap_min > 0.001: 1321\n",
      "# contexts with gap_min > 0.01: 1291\n",
      "# contexts with gap_min > 0.1: 1020\n",
      "Spanning R^24\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [95, 95, 8]\n",
      "Iteration 1, loss = 0.42494933\n",
      "Iteration 2, loss = 0.34072739\n",
      "Iteration 3, loss = 0.27328721\n",
      "Iteration 4, loss = 0.21762522\n",
      "Iteration 5, loss = 0.17949634\n",
      "Iteration 6, loss = 0.15165712\n",
      "Iteration 7, loss = 0.13177426\n",
      "Iteration 8, loss = 0.11665288\n",
      "Iteration 9, loss = 0.10491257\n",
      "Iteration 10, loss = 0.09557395\n",
      "Iteration 11, loss = 0.08857221\n",
      "Iteration 12, loss = 0.08175276\n",
      "Iteration 13, loss = 0.07624670\n",
      "Iteration 14, loss = 0.07074843\n",
      "Iteration 15, loss = 0.06707228\n",
      "Iteration 16, loss = 0.06436555\n",
      "Iteration 17, loss = 0.06166793\n",
      "Iteration 18, loss = 0.05842109\n",
      "Iteration 19, loss = 0.05721191\n",
      "Iteration 20, loss = 0.05359951\n",
      "Iteration 21, loss = 0.05252404\n",
      "Iteration 22, loss = 0.04933108\n",
      "Iteration 23, loss = 0.04828342\n",
      "Iteration 24, loss = 0.04727465\n",
      "Iteration 25, loss = 0.04645552\n",
      "Iteration 26, loss = 0.04487935\n",
      "Iteration 27, loss = 0.04268040\n",
      "Iteration 28, loss = 0.04281716\n",
      "Iteration 29, loss = 0.04186891\n",
      "Iteration 30, loss = 0.04134544\n",
      "Iteration 31, loss = 0.03920453\n",
      "Iteration 32, loss = 0.03925088\n",
      "Iteration 33, loss = 0.03988510\n",
      "Iteration 34, loss = 0.03862279\n",
      "Iteration 35, loss = 0.03803289\n",
      "Iteration 36, loss = 0.03701672\n",
      "Iteration 37, loss = 0.03653551\n",
      "Iteration 38, loss = 0.03585617\n",
      "Iteration 39, loss = 0.03537502\n",
      "Iteration 40, loss = 0.03436176\n",
      "Iteration 41, loss = 0.03440687\n",
      "Iteration 42, loss = 0.03436078\n",
      "Iteration 43, loss = 0.03291225\n",
      "Iteration 44, loss = 0.03206580\n",
      "Iteration 45, loss = 0.03255445\n",
      "Iteration 46, loss = 0.03123697\n",
      "Iteration 47, loss = 0.03199902\n",
      "Iteration 48, loss = 0.03097564\n",
      "Iteration 49, loss = 0.03117625\n",
      "Iteration 50, loss = 0.03028346\n",
      "Iteration 51, loss = 0.02947847\n",
      "Iteration 52, loss = 0.03040483\n",
      "Iteration 53, loss = 0.02858551\n",
      "Iteration 54, loss = 0.02810425\n",
      "Iteration 55, loss = 0.02932619\n",
      "Iteration 56, loss = 0.02957409\n",
      "Iteration 57, loss = 0.02804877\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 8): 0.6624333384748313\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.1255817445358386\n",
      "MSE (recomputed with last layer only): 0.12558174437965952\n",
      "d=9\n",
      "phi max norm: 55.2\n",
      "theta norm: 1.19\n",
      "MSE (mu): 0.12558174418937984\n",
      "mu: max 6.528271675109863 - min -0.6578641533851624\n",
      "gap max: 6.869101\n",
      "gap min: 0.00016331673\n",
      "# contexts with gap_min > 0.001: 1313\n",
      "# contexts with gap_min > 0.01: 1278\n",
      "# contexts with gap_min > 0.1: 1010\n",
      "Spanning R^8\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 40, 41, 42, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [50, 50, 36]\n",
      "Iteration 1, loss = 0.43155262\n",
      "Iteration 2, loss = 0.33724356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.27795809\n",
      "Iteration 4, loss = 0.23934965\n",
      "Iteration 5, loss = 0.21351340\n",
      "Iteration 6, loss = 0.19421344\n",
      "Iteration 7, loss = 0.18151016\n",
      "Iteration 8, loss = 0.17144629\n",
      "Iteration 9, loss = 0.16164324\n",
      "Iteration 10, loss = 0.15478831\n",
      "Iteration 11, loss = 0.14875048\n",
      "Iteration 12, loss = 0.14311132\n",
      "Iteration 13, loss = 0.14020088\n",
      "Iteration 14, loss = 0.13421639\n",
      "Iteration 15, loss = 0.13155554\n",
      "Iteration 16, loss = 0.12777917\n",
      "Iteration 17, loss = 0.12557700\n",
      "Iteration 18, loss = 0.12268598\n",
      "Iteration 19, loss = 0.11996395\n",
      "Iteration 20, loss = 0.11782678\n",
      "Iteration 21, loss = 0.11514037\n",
      "Iteration 22, loss = 0.11285913\n",
      "Iteration 23, loss = 0.11146527\n",
      "Iteration 24, loss = 0.10893189\n",
      "Iteration 25, loss = 0.10829161\n",
      "Iteration 26, loss = 0.10508877\n",
      "Iteration 27, loss = 0.10407498\n",
      "Iteration 28, loss = 0.10340182\n",
      "Iteration 29, loss = 0.09993927\n",
      "Iteration 30, loss = 0.10081987\n",
      "Iteration 31, loss = 0.09868670\n",
      "Iteration 32, loss = 0.09924704\n",
      "Iteration 33, loss = 0.09702144\n",
      "Iteration 34, loss = 0.09565152\n",
      "Iteration 35, loss = 0.09450925\n",
      "Iteration 36, loss = 0.09254511\n",
      "Iteration 37, loss = 0.09228189\n",
      "Iteration 38, loss = 0.09222062\n",
      "Iteration 39, loss = 0.09199185\n",
      "Iteration 40, loss = 0.08969499\n",
      "Iteration 41, loss = 0.08878317\n",
      "Iteration 42, loss = 0.08882614\n",
      "Iteration 43, loss = 0.08757433\n",
      "Iteration 44, loss = 0.08708776\n",
      "Iteration 45, loss = 0.08556367\n",
      "Iteration 46, loss = 0.08539529\n",
      "Iteration 47, loss = 0.08596600\n",
      "Iteration 48, loss = 0.08380046\n",
      "Iteration 49, loss = 0.08317247\n",
      "Iteration 50, loss = 0.08341093\n",
      "Iteration 51, loss = 0.08194567\n",
      "Iteration 52, loss = 0.08208145\n",
      "Iteration 53, loss = 0.08078371\n",
      "Iteration 54, loss = 0.07996073\n",
      "Iteration 55, loss = 0.08173173\n",
      "Iteration 56, loss = 0.08034511\n",
      "Iteration 57, loss = 0.07960081\n",
      "Iteration 58, loss = 0.07806686\n",
      "Iteration 59, loss = 0.07929839\n",
      "Iteration 60, loss = 0.07700544\n",
      "Iteration 61, loss = 0.07695966\n",
      "Iteration 62, loss = 0.07837561\n",
      "Iteration 63, loss = 0.07565691\n",
      "Iteration 64, loss = 0.07603665\n",
      "Iteration 65, loss = 0.07526113\n",
      "Iteration 66, loss = 0.07501943\n",
      "Iteration 67, loss = 0.07513249\n",
      "Iteration 68, loss = 0.07488913\n",
      "Iteration 69, loss = 0.07391596\n",
      "Iteration 70, loss = 0.07428575\n",
      "Iteration 71, loss = 0.07377646\n",
      "Iteration 72, loss = 0.07382615\n",
      "Iteration 73, loss = 0.07147149\n",
      "Iteration 74, loss = 0.07181496\n",
      "Iteration 75, loss = 0.07308050\n",
      "Iteration 76, loss = 0.07196959\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 36): 0.4870080719135023\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.22995321763923673\n",
      "MSE (recomputed with last layer only): 0.229953217574071\n",
      "d=37\n",
      "phi max norm: 64.27\n",
      "theta norm: 1.73\n",
      "MSE (mu): 0.2299532170060712\n",
      "mu: max 7.707622528076172 - min -0.9652969837188721\n",
      "gap max: 8.078111\n",
      "gap min: 0.00024938583\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1286\n",
      "# contexts with gap_min > 0.1: 937\n",
      "WARNING: Spanning R^37\n",
      "lambda HLS: 6.4869804e-05\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 40, 41, 42, 44, 46, 47, 48, 49, 50]\n",
      "Training NN -- Size [136, 28]\n",
      "Iteration 1, loss = 0.42465266\n",
      "Iteration 2, loss = 0.29680373\n",
      "Iteration 3, loss = 0.22907521\n",
      "Iteration 4, loss = 0.18266731\n",
      "Iteration 5, loss = 0.14923318\n",
      "Iteration 6, loss = 0.12329201\n",
      "Iteration 7, loss = 0.10447039\n",
      "Iteration 8, loss = 0.09152330\n",
      "Iteration 9, loss = 0.08134596\n",
      "Iteration 10, loss = 0.07331151\n",
      "Iteration 11, loss = 0.06688621\n",
      "Iteration 12, loss = 0.06176881\n",
      "Iteration 13, loss = 0.05719258\n",
      "Iteration 14, loss = 0.05316405\n",
      "Iteration 15, loss = 0.04995902\n",
      "Iteration 16, loss = 0.04621663\n",
      "Iteration 17, loss = 0.04419743\n",
      "Iteration 18, loss = 0.03999394\n",
      "Iteration 19, loss = 0.03816925\n",
      "Iteration 20, loss = 0.03540360\n",
      "Iteration 21, loss = 0.03328551\n",
      "Iteration 22, loss = 0.03289659\n",
      "Iteration 23, loss = 0.03128701\n",
      "Iteration 24, loss = 0.02980788\n",
      "Iteration 25, loss = 0.02845619\n",
      "Iteration 26, loss = 0.02741359\n",
      "Iteration 27, loss = 0.02675534\n",
      "Iteration 28, loss = 0.02617577\n",
      "Iteration 29, loss = 0.02508246\n",
      "Iteration 30, loss = 0.02493556\n",
      "Iteration 31, loss = 0.02366724\n",
      "Iteration 32, loss = 0.02265458\n",
      "Iteration 33, loss = 0.02184162\n",
      "Iteration 34, loss = 0.02289732\n",
      "Iteration 35, loss = 0.02244606\n",
      "Iteration 36, loss = 0.02040322\n",
      "Iteration 37, loss = 0.02041165\n",
      "Iteration 38, loss = 0.02068636\n",
      "Iteration 39, loss = 0.01946227\n",
      "Iteration 40, loss = 0.01925993\n",
      "Iteration 41, loss = 0.01879904\n",
      "Iteration 42, loss = 0.01858926\n",
      "Iteration 43, loss = 0.01843785\n",
      "Iteration 44, loss = 0.01838055\n",
      "Iteration 45, loss = 0.01742664\n",
      "Iteration 46, loss = 0.01716732\n",
      "Iteration 47, loss = 0.01695999\n",
      "Iteration 48, loss = 0.01656752\n",
      "Iteration 49, loss = 0.01703765\n",
      "Iteration 50, loss = 0.01574833\n",
      "Iteration 51, loss = 0.01550061\n",
      "Iteration 52, loss = 0.01515799\n",
      "Iteration 53, loss = 0.01550705\n",
      "Iteration 54, loss = 0.01570729\n",
      "Iteration 55, loss = 0.01454669\n",
      "Iteration 56, loss = 0.01478891\n",
      "Iteration 57, loss = 0.01480650\n",
      "Iteration 58, loss = 0.01412165\n",
      "Iteration 59, loss = 0.01393649\n",
      "Iteration 60, loss = 0.01432348\n",
      "Iteration 61, loss = 0.01345036\n",
      "Iteration 62, loss = 0.01284683\n",
      "Iteration 63, loss = 0.01374886\n",
      "Iteration 64, loss = 0.01309320\n",
      "Iteration 65, loss = 0.01246289\n",
      "Iteration 66, loss = 0.01324114\n",
      "Iteration 67, loss = 0.01281167\n",
      "Iteration 68, loss = 0.01237789\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 28): 0.8136799006724575\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06167693933165999\n",
      "MSE (recomputed with last layer only): 0.06167693934713061\n",
      "d=29\n",
      "phi max norm: 36.87\n",
      "theta norm: 1.52\n",
      "MSE (mu): 0.06167693906446282\n",
      "mu: max 6.842662334442139 - min -1.0006232261657715\n",
      "gap max: 7.324131\n",
      "gap min: 0.0004682541\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1293\n",
      "# contexts with gap_min > 0.1: 1029\n",
      "Spanning R^27\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 37, 40, 41, 42, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [139, 139, 46]\n",
      "Iteration 1, loss = 0.40490510\n",
      "Iteration 2, loss = 0.27112959\n",
      "Iteration 3, loss = 0.19129720\n",
      "Iteration 4, loss = 0.14177261\n",
      "Iteration 5, loss = 0.11044387\n",
      "Iteration 6, loss = 0.08991873\n",
      "Iteration 7, loss = 0.07650846\n",
      "Iteration 8, loss = 0.06787857\n",
      "Iteration 9, loss = 0.05984695\n",
      "Iteration 10, loss = 0.05540504\n",
      "Iteration 11, loss = 0.05246185\n",
      "Iteration 12, loss = 0.04610480\n",
      "Iteration 13, loss = 0.04396036\n",
      "Iteration 14, loss = 0.04270751\n",
      "Iteration 15, loss = 0.03904677\n",
      "Iteration 16, loss = 0.03861351\n",
      "Iteration 17, loss = 0.03637131\n",
      "Iteration 18, loss = 0.03463259\n",
      "Iteration 19, loss = 0.03490801\n",
      "Iteration 20, loss = 0.03380574\n",
      "Iteration 21, loss = 0.03102659\n",
      "Iteration 22, loss = 0.02932158\n",
      "Iteration 23, loss = 0.02883784\n",
      "Iteration 24, loss = 0.02784499\n",
      "Iteration 25, loss = 0.02845299\n",
      "Iteration 26, loss = 0.02821977\n",
      "Iteration 27, loss = 0.02531033\n",
      "Iteration 28, loss = 0.02576892\n",
      "Iteration 29, loss = 0.02433408\n",
      "Iteration 30, loss = 0.02355975\n",
      "Iteration 31, loss = 0.02310542\n",
      "Iteration 32, loss = 0.02431610\n",
      "Iteration 33, loss = 0.02244236\n",
      "Iteration 34, loss = 0.02238503\n",
      "Iteration 35, loss = 0.02122310\n",
      "Iteration 36, loss = 0.02195275\n",
      "Iteration 37, loss = 0.02086037\n",
      "Iteration 38, loss = 0.02140180\n",
      "Iteration 39, loss = 0.01943889\n",
      "Iteration 40, loss = 0.02006922\n",
      "Iteration 41, loss = 0.01991385\n",
      "Iteration 42, loss = 0.01873687\n",
      "Iteration 43, loss = 0.01820501\n",
      "Iteration 44, loss = 0.01906258\n",
      "Iteration 45, loss = 0.01837406\n",
      "Iteration 46, loss = 0.01749578\n",
      "Iteration 47, loss = 0.01742303\n",
      "Iteration 48, loss = 0.01750017\n",
      "Iteration 49, loss = 0.01725306\n",
      "Iteration 50, loss = 0.01704085\n",
      "Iteration 51, loss = 0.01636648\n",
      "Iteration 52, loss = 0.01568532\n",
      "Iteration 53, loss = 0.01667896\n",
      "Iteration 54, loss = 0.01675513\n",
      "Iteration 55, loss = 0.01561625\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 46): 0.7628135380411641\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.07854246339256847\n",
      "MSE (recomputed with last layer only): 0.07854246333100767\n",
      "d=47\n",
      "phi max norm: 85.95\n",
      "theta norm: 1.15\n",
      "MSE (mu): 0.07854246286395475\n",
      "mu: max 6.88519287109375 - min -0.6594170928001404\n",
      "gap max: 7.277993\n",
      "gap min: 0.00029802322\n",
      "# contexts with gap_min > 0.001: 1316\n",
      "# contexts with gap_min > 0.01: 1288\n",
      "# contexts with gap_min > 0.1: 1006\n",
      "Spanning R^43\n",
      "lambda HLS: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 40, 41, 42, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [176, 176, 37]\n",
      "Iteration 1, loss = 0.40819522\n",
      "Iteration 2, loss = 0.27163729\n",
      "Iteration 3, loss = 0.18021263\n",
      "Iteration 4, loss = 0.12517710\n",
      "Iteration 5, loss = 0.09422983\n",
      "Iteration 6, loss = 0.07465787\n",
      "Iteration 7, loss = 0.06319286\n",
      "Iteration 8, loss = 0.05547870\n",
      "Iteration 9, loss = 0.04929311\n",
      "Iteration 10, loss = 0.04712947\n",
      "Iteration 11, loss = 0.04250448\n",
      "Iteration 12, loss = 0.03898153\n",
      "Iteration 13, loss = 0.03849628\n",
      "Iteration 14, loss = 0.03465797\n",
      "Iteration 15, loss = 0.03362038\n",
      "Iteration 16, loss = 0.03445195\n",
      "Iteration 17, loss = 0.03013601\n",
      "Iteration 18, loss = 0.02838429\n",
      "Iteration 19, loss = 0.02852175\n",
      "Iteration 20, loss = 0.02766529\n",
      "Iteration 21, loss = 0.02574474\n",
      "Iteration 22, loss = 0.02668052\n",
      "Iteration 23, loss = 0.02460395\n",
      "Iteration 24, loss = 0.02283955\n",
      "Iteration 25, loss = 0.02417340\n",
      "Iteration 26, loss = 0.02141474\n",
      "Iteration 27, loss = 0.02128289\n",
      "Iteration 28, loss = 0.02162061\n",
      "Iteration 29, loss = 0.02123945\n",
      "Iteration 30, loss = 0.01926451\n",
      "Iteration 31, loss = 0.02073990\n",
      "Iteration 32, loss = 0.01892976\n",
      "Iteration 33, loss = 0.01925698\n",
      "Iteration 34, loss = 0.01789854\n",
      "Iteration 35, loss = 0.01674962\n",
      "Iteration 36, loss = 0.01812749\n",
      "Iteration 37, loss = 0.01818548\n",
      "Iteration 38, loss = 0.01690091\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 37): 0.7928604595623668\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.07432362678079252\n",
      "MSE (recomputed with last layer only): 0.07432362678950435\n",
      "d=38\n",
      "phi max norm: 93.46\n",
      "theta norm: 0.93\n",
      "MSE (mu): 0.0743236258408894\n",
      "mu: max 6.242449760437012 - min -0.7028054594993591\n",
      "gap max: 6.5655146\n",
      "gap min: 0.00044727325\n",
      "# contexts with gap_min > 0.001: 1317\n",
      "# contexts with gap_min > 0.01: 1289\n",
      "# contexts with gap_min > 0.1: 979\n",
      "Spanning R^35\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 40, 41, 42, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [160, 20]\n",
      "Iteration 1, loss = 0.41147111\n",
      "Iteration 2, loss = 0.29057137\n",
      "Iteration 3, loss = 0.20480751\n",
      "Iteration 4, loss = 0.15074675\n",
      "Iteration 5, loss = 0.11701650\n",
      "Iteration 6, loss = 0.09657098\n",
      "Iteration 7, loss = 0.08291758\n",
      "Iteration 8, loss = 0.07377816\n",
      "Iteration 9, loss = 0.06552719\n",
      "Iteration 10, loss = 0.06103269\n",
      "Iteration 11, loss = 0.05532104\n",
      "Iteration 12, loss = 0.05132879\n",
      "Iteration 13, loss = 0.04802826\n",
      "Iteration 14, loss = 0.04613339\n",
      "Iteration 15, loss = 0.04276293\n",
      "Iteration 16, loss = 0.03966696\n",
      "Iteration 17, loss = 0.03798115\n",
      "Iteration 18, loss = 0.03536002\n",
      "Iteration 19, loss = 0.03311689\n",
      "Iteration 20, loss = 0.03195197\n",
      "Iteration 21, loss = 0.02989461\n",
      "Iteration 22, loss = 0.02837063\n",
      "Iteration 23, loss = 0.02736199\n",
      "Iteration 24, loss = 0.02637237\n",
      "Iteration 25, loss = 0.02521460\n",
      "Iteration 26, loss = 0.02484696\n",
      "Iteration 27, loss = 0.02326098\n",
      "Iteration 28, loss = 0.02297850\n",
      "Iteration 29, loss = 0.02236300\n",
      "Iteration 30, loss = 0.02110692\n",
      "Iteration 31, loss = 0.02059001\n",
      "Iteration 32, loss = 0.02073090\n",
      "Iteration 33, loss = 0.01994120\n",
      "Iteration 34, loss = 0.01916052\n",
      "Iteration 35, loss = 0.01925479\n",
      "Iteration 36, loss = 0.01875602\n",
      "Iteration 37, loss = 0.01826068\n",
      "Iteration 38, loss = 0.01777055\n",
      "Iteration 39, loss = 0.01713378\n",
      "Iteration 40, loss = 0.01769115\n",
      "Iteration 41, loss = 0.01712828\n",
      "Iteration 42, loss = 0.01603681\n",
      "Iteration 43, loss = 0.01588616\n",
      "Iteration 44, loss = 0.01618490\n",
      "Iteration 45, loss = 0.01536932\n",
      "Iteration 46, loss = 0.01486749\n",
      "Iteration 47, loss = 0.01466529\n",
      "Iteration 48, loss = 0.01545089\n",
      "Iteration 49, loss = 0.01436689\n",
      "Iteration 50, loss = 0.01382257\n",
      "Iteration 51, loss = 0.01403921\n",
      "Iteration 52, loss = 0.01382950\n",
      "Iteration 53, loss = 0.01383802\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 20): 0.8140350576396311\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06423984965069167\n",
      "MSE (recomputed with last layer only): 0.0642398497016509\n",
      "d=21\n",
      "phi max norm: 33.56\n",
      "theta norm: 1.37\n",
      "MSE (mu): 0.06423985020112406\n",
      "mu: max 8.079967498779297 - min -0.5814231038093567\n",
      "gap max: 8.40665\n",
      "gap min: 0.00011610985\n",
      "# contexts with gap_min > 0.001: 1318\n",
      "# contexts with gap_min > 0.01: 1293\n",
      "# contexts with gap_min > 0.1: 1047\n",
      "Spanning R^19\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 40, 41, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [188, 188, 42]\n",
      "Iteration 1, loss = 0.39269932\n",
      "Iteration 2, loss = 0.25545947\n",
      "Iteration 3, loss = 0.17050741\n",
      "Iteration 4, loss = 0.12091470\n",
      "Iteration 5, loss = 0.08976717\n",
      "Iteration 6, loss = 0.07393101\n",
      "Iteration 7, loss = 0.05946237\n",
      "Iteration 8, loss = 0.05484353\n",
      "Iteration 9, loss = 0.04894714\n",
      "Iteration 10, loss = 0.04373744\n",
      "Iteration 11, loss = 0.04150179\n",
      "Iteration 12, loss = 0.03825851\n",
      "Iteration 13, loss = 0.03625904\n",
      "Iteration 14, loss = 0.03561877\n",
      "Iteration 15, loss = 0.03242504\n",
      "Iteration 16, loss = 0.02973638\n",
      "Iteration 17, loss = 0.02921337\n",
      "Iteration 18, loss = 0.02803710\n",
      "Iteration 19, loss = 0.02875371\n",
      "Iteration 20, loss = 0.02638078\n",
      "Iteration 21, loss = 0.02527283\n",
      "Iteration 22, loss = 0.02561309\n",
      "Iteration 23, loss = 0.02314959\n",
      "Iteration 24, loss = 0.02308441\n",
      "Iteration 25, loss = 0.02164935\n",
      "Iteration 26, loss = 0.02110504\n",
      "Iteration 27, loss = 0.02181622\n",
      "Iteration 28, loss = 0.02159998\n",
      "Iteration 29, loss = 0.01955305\n",
      "Iteration 30, loss = 0.02008326\n",
      "Iteration 31, loss = 0.01853560\n",
      "Iteration 32, loss = 0.01845901\n",
      "Iteration 33, loss = 0.01889908\n",
      "Iteration 34, loss = 0.01677797\n",
      "Iteration 35, loss = 0.01752651\n",
      "Iteration 36, loss = 0.01581695\n",
      "Iteration 37, loss = 0.01582355\n",
      "Iteration 38, loss = 0.01671607\n",
      "Iteration 39, loss = 0.01722988\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 42): 0.8003031599722945\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.07096666927891561\n",
      "MSE (recomputed with last layer only): 0.07096666926090704\n",
      "d=43\n",
      "phi max norm: 89.03\n",
      "theta norm: 0.89\n",
      "MSE (mu): 0.0709666694276328\n",
      "mu: max 7.054773807525635 - min -0.5133576393127441\n",
      "gap max: 7.3916693\n",
      "gap min: 0.0007479191\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1289\n",
      "# contexts with gap_min > 0.1: 970\n",
      "Spanning R^31\n",
      "lambda HLS: 0.0\n",
      "\n",
      "[5, 6, 10, 11, 12, 15, 16, 17, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 40, 41, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [75, 14]\n",
      "Iteration 1, loss = 0.46322215\n",
      "Iteration 2, loss = 0.33991117\n",
      "Iteration 3, loss = 0.28503017\n",
      "Iteration 4, loss = 0.24343415\n",
      "Iteration 5, loss = 0.21266607\n",
      "Iteration 6, loss = 0.18712114\n",
      "Iteration 7, loss = 0.16775392\n",
      "Iteration 8, loss = 0.15370028\n",
      "Iteration 9, loss = 0.14133247\n",
      "Iteration 10, loss = 0.13134411\n",
      "Iteration 11, loss = 0.12510373\n",
      "Iteration 12, loss = 0.11761704\n",
      "Iteration 13, loss = 0.11185367\n",
      "Iteration 14, loss = 0.10743786\n",
      "Iteration 15, loss = 0.10286143\n",
      "Iteration 16, loss = 0.09927611\n",
      "Iteration 17, loss = 0.09567320\n",
      "Iteration 18, loss = 0.09134642\n",
      "Iteration 19, loss = 0.08804627\n",
      "Iteration 20, loss = 0.08580405\n",
      "Iteration 21, loss = 0.08231268\n",
      "Iteration 22, loss = 0.07983215\n",
      "Iteration 23, loss = 0.07772805\n",
      "Iteration 24, loss = 0.07477475\n",
      "Iteration 25, loss = 0.07342428\n",
      "Iteration 26, loss = 0.07196003\n",
      "Iteration 27, loss = 0.07046881\n",
      "Iteration 28, loss = 0.06843195\n",
      "Iteration 29, loss = 0.06739320\n",
      "Iteration 30, loss = 0.06614272\n",
      "Iteration 31, loss = 0.06464436\n",
      "Iteration 32, loss = 0.06334313\n",
      "Iteration 33, loss = 0.06245330\n",
      "Iteration 34, loss = 0.06120302\n",
      "Iteration 35, loss = 0.06035344\n",
      "Iteration 36, loss = 0.05909599\n",
      "Iteration 37, loss = 0.05852869\n",
      "Iteration 38, loss = 0.05808376\n",
      "Iteration 39, loss = 0.05770795\n",
      "Iteration 40, loss = 0.05587061\n",
      "Iteration 41, loss = 0.05441087\n",
      "Iteration 42, loss = 0.05480577\n",
      "Iteration 43, loss = 0.05350377\n",
      "Iteration 44, loss = 0.05235254\n",
      "Iteration 45, loss = 0.05155495\n",
      "Iteration 46, loss = 0.05076361\n",
      "Iteration 47, loss = 0.05021167\n",
      "Iteration 48, loss = 0.04953748\n",
      "Iteration 49, loss = 0.04835866\n",
      "Iteration 50, loss = 0.04821515\n",
      "Iteration 51, loss = 0.04838670\n",
      "Iteration 52, loss = 0.04712995\n",
      "Iteration 53, loss = 0.04658656\n",
      "Iteration 54, loss = 0.04564933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.04618670\n",
      "Iteration 56, loss = 0.04561178\n",
      "Iteration 57, loss = 0.04428686\n",
      "Iteration 58, loss = 0.04435514\n",
      "Iteration 59, loss = 0.04331247\n",
      "Iteration 60, loss = 0.04386392\n",
      "Iteration 61, loss = 0.04291931\n",
      "Iteration 62, loss = 0.04187090\n",
      "Iteration 63, loss = 0.04137033\n",
      "Iteration 64, loss = 0.04166160\n",
      "Iteration 65, loss = 0.04142098\n",
      "Iteration 66, loss = 0.04063393\n",
      "Iteration 67, loss = 0.04003497\n",
      "Iteration 68, loss = 0.03979547\n",
      "Iteration 69, loss = 0.03975977\n",
      "Iteration 70, loss = 0.03950337\n",
      "Iteration 71, loss = 0.03757253\n",
      "Iteration 72, loss = 0.03859849\n",
      "Iteration 73, loss = 0.03798220\n",
      "Iteration 74, loss = 0.03703102\n",
      "Iteration 75, loss = 0.03724178\n",
      "Iteration 76, loss = 0.03627981\n",
      "Iteration 77, loss = 0.03645597\n",
      "Iteration 78, loss = 0.03689822\n",
      "Iteration 79, loss = 0.03552495\n",
      "Iteration 80, loss = 0.03546803\n",
      "Iteration 81, loss = 0.03495024\n",
      "Iteration 82, loss = 0.03495869\n",
      "Iteration 83, loss = 0.03430468\n",
      "Iteration 84, loss = 0.03498860\n",
      "Iteration 85, loss = 0.03414380\n",
      "Iteration 86, loss = 0.03349824\n",
      "Iteration 87, loss = 0.03336868\n",
      "Iteration 88, loss = 0.03301220\n",
      "Iteration 89, loss = 0.03357935\n",
      "Iteration 90, loss = 0.03271935\n",
      "Iteration 91, loss = 0.03337616\n",
      "Iteration 92, loss = 0.03277264\n",
      "Iteration 93, loss = 0.03244590\n",
      "Iteration 94, loss = 0.03199347\n",
      "Iteration 95, loss = 0.03168670\n",
      "Iteration 96, loss = 0.03196184\n",
      "Iteration 97, loss = 0.03138730\n",
      "Iteration 98, loss = 0.03151837\n",
      "Iteration 99, loss = 0.03132607\n",
      "Iteration 100, loss = 0.03141272\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 14): 0.6476603775312711\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.13113766812384325\n",
      "MSE (recomputed with last layer only): 0.13113766801079976\n",
      "d=15\n",
      "phi max norm: 41.9\n",
      "theta norm: 1.92\n",
      "MSE (mu): 0.13113767013507416\n",
      "mu: max 6.492524147033691 - min -0.9595465660095215\n",
      "gap max: 6.813684\n",
      "gap min: 0.00016212463\n",
      "# contexts with gap_min > 0.001: 1318\n",
      "# contexts with gap_min > 0.01: 1274\n",
      "# contexts with gap_min > 0.1: 963\n",
      "WARNING: Spanning R^15\n",
      "lambda HLS: 0.017474622\n",
      "\n",
      "[5, 6, 10, 11, 15, 16, 17, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 34, 35, 40, 41, 44, 47, 48, 49, 50]\n",
      "Training NN -- Size [84, 12]\n",
      "Iteration 1, loss = 0.42339804\n",
      "Iteration 2, loss = 0.31826825\n",
      "Iteration 3, loss = 0.25778770\n",
      "Iteration 4, loss = 0.21482708\n",
      "Iteration 5, loss = 0.18288693\n",
      "Iteration 6, loss = 0.16079015\n",
      "Iteration 7, loss = 0.14487564\n",
      "Iteration 8, loss = 0.13135699\n",
      "Iteration 9, loss = 0.12113673\n",
      "Iteration 10, loss = 0.11376428\n",
      "Iteration 11, loss = 0.10780843\n",
      "Iteration 12, loss = 0.10124396\n",
      "Iteration 13, loss = 0.09668484\n",
      "Iteration 14, loss = 0.09263426\n",
      "Iteration 15, loss = 0.08812681\n",
      "Iteration 16, loss = 0.08551405\n",
      "Iteration 17, loss = 0.08235759\n",
      "Iteration 18, loss = 0.08011657\n",
      "Iteration 19, loss = 0.07709039\n",
      "Iteration 20, loss = 0.07530588\n",
      "Iteration 21, loss = 0.07308588\n",
      "Iteration 22, loss = 0.07097168\n",
      "Iteration 23, loss = 0.06960529\n",
      "Iteration 24, loss = 0.06791729\n",
      "Iteration 25, loss = 0.06605795\n",
      "Iteration 26, loss = 0.06518534\n",
      "Iteration 27, loss = 0.06341081\n",
      "Iteration 28, loss = 0.06283304\n",
      "Iteration 29, loss = 0.06064976\n",
      "Iteration 30, loss = 0.05995520\n",
      "Iteration 31, loss = 0.05842235\n",
      "Iteration 32, loss = 0.05771951\n",
      "Iteration 33, loss = 0.05663358\n",
      "Iteration 34, loss = 0.05592074\n",
      "Iteration 35, loss = 0.05498488\n",
      "Iteration 36, loss = 0.05384707\n",
      "Iteration 37, loss = 0.05293351\n",
      "Iteration 38, loss = 0.05230193\n",
      "Iteration 39, loss = 0.05174097\n",
      "Iteration 40, loss = 0.05049551\n",
      "Iteration 41, loss = 0.05015320\n",
      "Iteration 42, loss = 0.04942386\n",
      "Iteration 43, loss = 0.04851259\n",
      "Iteration 44, loss = 0.04748800\n",
      "Iteration 45, loss = 0.04703984\n",
      "Iteration 46, loss = 0.04638149\n",
      "Iteration 47, loss = 0.04580484\n",
      "Iteration 48, loss = 0.04521937\n",
      "Iteration 49, loss = 0.04432920\n",
      "Iteration 50, loss = 0.04385951\n",
      "Iteration 51, loss = 0.04282760\n",
      "Iteration 52, loss = 0.04273030\n",
      "Iteration 53, loss = 0.04194305\n",
      "Iteration 54, loss = 0.04166350\n",
      "Iteration 55, loss = 0.04118518\n",
      "Iteration 56, loss = 0.04055207\n",
      "Iteration 57, loss = 0.04008261\n",
      "Iteration 58, loss = 0.03877750\n",
      "Iteration 59, loss = 0.03850189\n",
      "Iteration 60, loss = 0.03792902\n",
      "Iteration 61, loss = 0.03692061\n",
      "Iteration 62, loss = 0.03731568\n",
      "Iteration 63, loss = 0.03720320\n",
      "Iteration 64, loss = 0.03608096\n",
      "Iteration 65, loss = 0.03605190\n",
      "Iteration 66, loss = 0.03556258\n",
      "Iteration 67, loss = 0.03520603\n",
      "Iteration 68, loss = 0.03500572\n",
      "Iteration 69, loss = 0.03419538\n",
      "Iteration 70, loss = 0.03386975\n",
      "Iteration 71, loss = 0.03412531\n",
      "Iteration 72, loss = 0.03379727\n",
      "Iteration 73, loss = 0.03340484\n",
      "Iteration 74, loss = 0.03309962\n",
      "Iteration 75, loss = 0.03269969\n",
      "Iteration 76, loss = 0.03245611\n",
      "Iteration 77, loss = 0.03226436\n",
      "Iteration 78, loss = 0.03185911\n",
      "Iteration 79, loss = 0.03181544\n",
      "Iteration 80, loss = 0.03210042\n",
      "Iteration 81, loss = 0.03083402\n",
      "Iteration 82, loss = 0.03090459\n",
      "Iteration 83, loss = 0.03113361\n",
      "Iteration 84, loss = 0.03081653\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 12): 0.6563112854810313\n",
      "Best so so far: d=9 with R^2: 0.8316372687795281\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.1294628891620364\n",
      "MSE (recomputed with last layer only): 0.12946288895056457\n",
      "d=13\n",
      "phi max norm: 27.82\n",
      "theta norm: 1.87\n",
      "MSE (mu): 0.1294628904299292\n",
      "mu: max 8.76286506652832 - min -0.3388788104057312\n",
      "gap max: 9.083017\n",
      "gap min: 0.001155138\n",
      "# contexts with gap_min > 0.001: 1322\n",
      "# contexts with gap_min > 0.01: 1291\n",
      "# contexts with gap_min > 0.1: 1059\n",
      "WARNING: Spanning R^13\n",
      "lambda HLS: 0.02360433\n",
      "\n",
      "Maximum R^2: 0.8316372687795281 - d=9\n"
     ]
    }
   ],
   "source": [
    "# fit networks\n",
    "\n",
    "# hidden = [256, 256]\n",
    "# ds = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "test_size=0.25\n",
    "\n",
    "hidden_low = 50\n",
    "hidden_high = 200\n",
    "ds = np.arange(46) + 5\n",
    "ds = ds.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "best_d = None\n",
    "max_score = 0\n",
    "\n",
    "n_models = 20\n",
    "\n",
    "for i in range(n_models):\n",
    "    hidden = np.random.randint(hidden_low, hidden_high)\n",
    "    d = np.random.choice(ds)\n",
    "    ds.remove(d)\n",
    "    print(ds)\n",
    "    \n",
    "    size = [hidden]\n",
    "    if np.random.randint(2) == 1:\n",
    "        size += [hidden]\n",
    "    size += [d]\n",
    "\n",
    "    print(\"Training NN -- Size {0}\".format(size))\n",
    "    net = MLPRegressor(hidden_layer_sizes=size, max_iter=500, verbose=True).fit(X_train, y_train)\n",
    "    score = net.score(X_test, y_test)\n",
    "    print(\"R^2 (size {0}): {1}\".format(d, score))\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        best_d = d\n",
    "    print(\"Best so so far: d={0} with R^2: {1}\".format(best_d, max_score))\n",
    "    print()\n",
    "    print(\"Saving model...\")\n",
    "    save_model(net)\n",
    "    del(net)\n",
    "    print()\n",
    "\n",
    "print(\"Maximum R^2: {0} - d={1}\".format(max_score, best_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
