{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spanrd(vectors, d):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - vectors (array): matrix (N, d)\n",
    "        - d (int): dimension of the space to be spanned\n",
    "    Return:\n",
    "        - True or False\n",
    "    \"\"\"\n",
    "    # https://math.stackexchange.com/questions/56201/how-to-tell-if-a-set-of-vectors-spans-a-space\n",
    "    # https://stackoverflow.com/questions/15638650/is-there-a-standard-solution-for-gauss-elimination-in-python\n",
    "    pl, u = lu(vectors, permute_l=True)\n",
    "    rank = np.linalg.matrix_rank(u)\n",
    "    return d == int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (1322, 220)\n",
      "ratings: max 6.648947490095273 - min -0.3259578617346158\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data_path = \"lastfmlog.npy\"\n",
    "\n",
    "ratings = np.load(data_path)\n",
    "#print(np.mean(ratings), np.sum(ratings > 0), ratings.size)\n",
    "ratings = (ratings - np.mean(ratings)) / np.std(ratings)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ratings.flatten())\n",
    "plt.show()\n",
    "\n",
    "print(\"Loaded dataset: {}\".format(ratings.shape))\n",
    "\n",
    "n_users, n_items = ratings.shape\n",
    "print(\"ratings: max {0} - min {1}\".format(ratings.max(), ratings.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  150\n",
      "RMSE: 0.2765351286203227\n",
      "MAX_ERR: 2.876380863604467\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "\n",
    "K = 150\n",
    "U, s, Vt = svds(ratings, k=K)\n",
    "s = np.diag(s)\n",
    "U = np.dot(U, s)\n",
    "\n",
    "# MSE\n",
    "Yhat = U.dot(Vt)\n",
    "rmse = np.sqrt(np.mean(np.abs(Yhat - ratings) ** 2))\n",
    "print(\"K: \", K)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAX_ERR:\", np.abs(Yhat - ratings).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(net, normalize=False):\n",
    "\n",
    "    # Build features\n",
    "    X_pred = X\n",
    "\n",
    "    hidden_layer_sizes = list(net.hidden_layer_sizes)\n",
    "\n",
    "    layer_units = [X_pred.shape[1]] + hidden_layer_sizes + [1]\n",
    "    activations = [X_pred]\n",
    "    for i in range(net.n_layers_ - 1):\n",
    "        activations.append(np.empty((X_pred.shape[0], layer_units[i + 1])))\n",
    "\n",
    "    net._forward_pass(activations)\n",
    "    y_pred = activations[-1]\n",
    "    print(\"MSE (original):\", np.mean((y_pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get weights\n",
    "    last_w = net.coefs_[-1]\n",
    "    bias = np.array(net.intercepts_[-1]).reshape((1, 1))\n",
    "    last_w = np.concatenate([last_w, bias])\n",
    "\n",
    "    # get last-layer features\n",
    "    last_feat = np.array(activations[-2], dtype=np.float32)\n",
    "    last_feat = np.concatenate([last_feat, np.ones((X_pred.shape[0], 1))], axis=1)\n",
    "\n",
    "    # get prediction\n",
    "    pred = last_feat.dot(last_w)\n",
    "    print(\"MSE (recomputed with last layer only):\", np.mean((pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get feature matrix\n",
    "    d = hidden_layer_sizes[-1] + 1\n",
    "    print(\"d={0}\".format(d))\n",
    "    phi = np.empty((n_users, n_items, d), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for t in range(n_users):\n",
    "        for z in range(n_items):\n",
    "            phi[t, z, :] = last_feat[idx, :] / (np.linalg.norm(last_feat[idx, :]) if normalize else 1)\n",
    "            idx += 1\n",
    "    assert idx == last_feat.shape[0]\n",
    "\n",
    "    # get param\n",
    "    theta = np.array(last_w, dtype=np.float32).squeeze()\n",
    "    if normalize:\n",
    "        theta = theta / np.linalg.norm(theta)\n",
    "        \n",
    "    phi_norm = round(np.linalg.norm(phi, axis=2).max(), 2)\n",
    "    print(\"phi max norm:\", phi_norm)\n",
    "    theta_norm = round(np.linalg.norm(theta), 2)\n",
    "    print(\"theta norm:\", theta_norm)\n",
    "\n",
    "    # check predictions\n",
    "    mu = phi.dot(theta)\n",
    "    print(\"MSE (mu):\", np.mean(np.abs(ratings - mu).flatten()**2))\n",
    "    print(\"mu: max {0} - min {1}\".format(mu.max(), mu.min()))\n",
    "    gap = np.max(mu, axis=1)[:, np.newaxis] - mu\n",
    "    print(\"gap max:\", gap.max())\n",
    "    gap[gap == 0] = 100\n",
    "    print(\"gap min:\", gap.min())\n",
    "    gap = np.min(gap, axis=1)\n",
    "    print(\"# contexts with gap_min > 0.001:\", np.sum(gap > 0.001))\n",
    "    print(\"# contexts with gap_min > 0.01:\", np.sum(gap > 0.01))\n",
    "    print(\"# contexts with gap_min > 0.1:\", np.sum(gap > 0.1))\n",
    "\n",
    "    # check span\n",
    "    astar = np.argmax(mu, axis=1)\n",
    "    fstar = np.array([phi[x, astar[x]] for x in range(n_users)])\n",
    "\n",
    "    span = d\n",
    "    for i in range(d):\n",
    "        if check_spanrd(fstar, d - i):\n",
    "            span = d - i\n",
    "            break\n",
    "\n",
    "    print(\"{0}Spanning R^{1}\".format(\"WARNING: \" if span == d else \"\", span))\n",
    "    \n",
    "    # compute lambda HLS\n",
    "    \n",
    "    outer = np.matmul(fstar.T, fstar) / n_users\n",
    "    lambda_hls = np.linalg.eigvals(outer).min()\n",
    "    print(\"lambda HLS:\", lambda_hls)\n",
    "\n",
    "    # save\n",
    "#     np.savez_compressed('lastfm_d{0}_span{1}_L{2:.2f}_S{3:.2f}_hls{4:.5f}.npz'.format(d,span,phi_norm,theta_norm, lambda_hls), \n",
    "#                         features=phi, theta=theta)\n",
    "    np.savez_compressed('lastfm_d{0}_span{1}.npz'.format(d,span), features=phi, theta=theta)\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets\n",
    "\n",
    "X, y = [], []\n",
    "for t in range(n_users):\n",
    "    for z in range(n_items):\n",
    "        feat = np.concatenate([U[t], Vt[:, z]]).ravel()\n",
    "        X.append(feat)\n",
    "        y.append(ratings[t, z])\n",
    "X = np.array(X)\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN -- Size [256, 256, 5]\n",
      "Iteration 1, loss = 0.45689128\n",
      "Iteration 2, loss = 0.38335265\n",
      "Iteration 3, loss = 0.34842782\n",
      "Iteration 4, loss = 0.32881936\n",
      "Iteration 5, loss = 0.31504306\n",
      "Iteration 6, loss = 0.30293415\n",
      "Iteration 7, loss = 0.29500647\n",
      "Iteration 8, loss = 0.28910277\n",
      "Iteration 9, loss = 0.28543216\n",
      "Iteration 10, loss = 0.28012562\n",
      "Iteration 11, loss = 0.27489630\n",
      "Iteration 12, loss = 0.26910817\n",
      "Iteration 13, loss = 0.26474857\n",
      "Iteration 14, loss = 0.26150227\n",
      "Iteration 15, loss = 0.25755512\n",
      "Iteration 16, loss = 0.25324819\n",
      "Iteration 17, loss = 0.24799641\n",
      "Iteration 18, loss = 0.24380131\n",
      "Iteration 19, loss = 0.24290905\n",
      "Iteration 20, loss = 0.24271822\n",
      "Iteration 21, loss = 0.24157589\n",
      "Iteration 22, loss = 0.24229899\n",
      "Iteration 23, loss = 0.24050313\n",
      "Iteration 24, loss = 0.23929988\n",
      "Iteration 25, loss = 0.23883341\n",
      "Iteration 26, loss = 0.23722694\n",
      "Iteration 27, loss = 0.23421742\n",
      "Iteration 28, loss = 0.23102474\n",
      "Iteration 29, loss = 0.22870129\n",
      "Iteration 30, loss = 0.22650166\n",
      "Iteration 31, loss = 0.22606735\n",
      "Iteration 32, loss = 0.22538862\n",
      "Iteration 33, loss = 0.22574928\n",
      "Iteration 34, loss = 0.22539629\n",
      "Iteration 35, loss = 0.22509982\n",
      "Iteration 36, loss = 0.22397990\n",
      "Iteration 37, loss = 0.22352898\n",
      "Iteration 38, loss = 0.22456278\n",
      "Iteration 39, loss = 0.22385381\n",
      "Iteration 40, loss = 0.22365663\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 5): 0.49374876215418445\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.45883330483514134\n",
      "MSE (recomputed with last layer only): 0.45883330481917517\n",
      "d=6\n",
      "phi max norm: 28.13\n",
      "theta norm: 0.3\n",
      "MSE (mu): 0.4588333042663146\n",
      "mu: max 6.4746880531311035 - min -0.17847375571727753\n",
      "gap max: 6.653162\n",
      "gap min: 3.4570694e-05\n",
      "# contexts with gap_min > 0.001: 1319\n",
      "# contexts with gap_min > 0.01: 1298\n",
      "# contexts with gap_min > 0.1: 1060\n",
      "Spanning R^2\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 10]\n",
      "Iteration 1, loss = 0.42180371\n",
      "Iteration 2, loss = 0.26206908\n",
      "Iteration 3, loss = 0.15008030\n",
      "Iteration 4, loss = 0.09515786\n",
      "Iteration 5, loss = 0.06856202\n",
      "Iteration 6, loss = 0.05724158\n",
      "Iteration 7, loss = 0.05181489\n",
      "Iteration 8, loss = 0.04435224\n",
      "Iteration 9, loss = 0.04130298\n",
      "Iteration 10, loss = 0.03904667\n",
      "Iteration 11, loss = 0.03479818\n",
      "Iteration 12, loss = 0.03334905\n",
      "Iteration 13, loss = 0.03175551\n",
      "Iteration 14, loss = 0.03213497\n",
      "Iteration 15, loss = 0.02771480\n",
      "Iteration 16, loss = 0.02768411\n",
      "Iteration 17, loss = 0.02667319\n",
      "Iteration 18, loss = 0.02554170\n",
      "Iteration 19, loss = 0.02483776\n",
      "Iteration 20, loss = 0.02476663\n",
      "Iteration 21, loss = 0.02452451\n",
      "Iteration 22, loss = 0.02301729\n",
      "Iteration 23, loss = 0.02067228\n",
      "Iteration 24, loss = 0.02215436\n",
      "Iteration 25, loss = 0.02128802\n",
      "Iteration 26, loss = 0.01961534\n",
      "Iteration 27, loss = 0.01829797\n",
      "Iteration 28, loss = 0.01921805\n",
      "Iteration 29, loss = 0.01976586\n",
      "Iteration 30, loss = 0.01679237\n",
      "Iteration 31, loss = 0.01667701\n",
      "Iteration 32, loss = 0.01707527\n",
      "Iteration 33, loss = 0.01851010\n",
      "Iteration 34, loss = 0.01568127\n",
      "Iteration 35, loss = 0.01530300\n",
      "Iteration 36, loss = 0.01580801\n",
      "Iteration 37, loss = 0.01418470\n",
      "Iteration 38, loss = 0.01585813\n",
      "Iteration 39, loss = 0.01551507\n",
      "Iteration 40, loss = 0.01332584\n",
      "Iteration 41, loss = 0.01488642\n",
      "Iteration 42, loss = 0.01316401\n",
      "Iteration 43, loss = 0.01351553\n",
      "Iteration 44, loss = 0.01284837\n",
      "Iteration 45, loss = 0.01358795\n",
      "Iteration 46, loss = 0.01250992\n",
      "Iteration 47, loss = 0.01275652\n",
      "Iteration 48, loss = 0.01314423\n",
      "Iteration 49, loss = 0.01125417\n",
      "Iteration 50, loss = 0.01275507\n",
      "Iteration 51, loss = 0.01176658\n",
      "Iteration 52, loss = 0.01394202\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 10): 0.8381779972663232\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.05503259927392245\n",
      "MSE (recomputed with last layer only): 0.055032599292941166\n",
      "d=11\n",
      "phi max norm: 120.56\n",
      "theta norm: 0.59\n",
      "MSE (mu): 0.05503259822803698\n",
      "mu: max 6.709043502807617 - min -0.6858648657798767\n",
      "gap max: 7.0461287\n",
      "gap min: 5.1021576e-05\n",
      "# contexts with gap_min > 0.001: 1317\n",
      "# contexts with gap_min > 0.01: 1285\n",
      "# contexts with gap_min > 0.1: 957\n",
      "Spanning R^7\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 15]\n",
      "Iteration 1, loss = 0.41418366\n",
      "Iteration 2, loss = 0.28548696\n",
      "Iteration 3, loss = 0.16548461\n",
      "Iteration 4, loss = 0.10030801\n",
      "Iteration 5, loss = 0.07149118\n",
      "Iteration 6, loss = 0.05371296\n",
      "Iteration 7, loss = 0.04428687\n",
      "Iteration 8, loss = 0.03739248\n",
      "Iteration 9, loss = 0.03432873\n",
      "Iteration 10, loss = 0.03690059\n",
      "Iteration 11, loss = 0.03425157\n",
      "Iteration 12, loss = 0.03013024\n",
      "Iteration 13, loss = 0.02880933\n",
      "Iteration 14, loss = 0.02962030\n",
      "Iteration 15, loss = 0.02770584\n",
      "Iteration 16, loss = 0.02684488\n",
      "Iteration 17, loss = 0.02296169\n",
      "Iteration 18, loss = 0.02402593\n",
      "Iteration 19, loss = 0.02238749\n",
      "Iteration 20, loss = 0.02283150\n",
      "Iteration 21, loss = 0.02196618\n",
      "Iteration 22, loss = 0.02163517\n",
      "Iteration 23, loss = 0.01943202\n",
      "Iteration 24, loss = 0.01894729\n",
      "Iteration 25, loss = 0.01874574\n",
      "Iteration 26, loss = 0.02013342\n",
      "Iteration 27, loss = 0.01750313\n",
      "Iteration 28, loss = 0.01768327\n",
      "Iteration 29, loss = 0.01551929\n",
      "Iteration 30, loss = 0.01509928\n",
      "Iteration 31, loss = 0.01631082\n",
      "Iteration 32, loss = 0.01561289\n",
      "Iteration 33, loss = 0.01659745\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 15): 0.8330227787256201\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06025842373229518\n",
      "MSE (recomputed with last layer only): 0.06025842362501974\n",
      "d=16\n",
      "phi max norm: 80.16\n",
      "theta norm: 0.57\n",
      "MSE (mu): 0.060258424055115584\n",
      "mu: max 6.200510501861572 - min -0.3307454586029053\n",
      "gap max: 6.5261717\n",
      "gap min: 9.36985e-05\n",
      "# contexts with gap_min > 0.001: 1316\n",
      "# contexts with gap_min > 0.01: 1276\n",
      "# contexts with gap_min > 0.1: 981\n",
      "Spanning R^6\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 20]\n",
      "Iteration 1, loss = 0.39955041\n",
      "Iteration 2, loss = 0.25744298\n",
      "Iteration 3, loss = 0.16006119\n",
      "Iteration 4, loss = 0.11111043\n",
      "Iteration 5, loss = 0.08247317\n",
      "Iteration 6, loss = 0.06353982\n",
      "Iteration 7, loss = 0.05428800\n",
      "Iteration 8, loss = 0.04771323\n",
      "Iteration 9, loss = 0.04281816\n",
      "Iteration 10, loss = 0.04048088\n",
      "Iteration 11, loss = 0.03668578\n",
      "Iteration 12, loss = 0.03490235\n",
      "Iteration 13, loss = 0.03346382\n",
      "Iteration 14, loss = 0.03150820\n",
      "Iteration 15, loss = 0.03067457\n",
      "Iteration 16, loss = 0.02866506\n",
      "Iteration 17, loss = 0.02668277\n",
      "Iteration 18, loss = 0.02557551\n",
      "Iteration 19, loss = 0.02615111\n",
      "Iteration 20, loss = 0.02512673\n",
      "Iteration 21, loss = 0.02230956\n",
      "Iteration 22, loss = 0.02382610\n",
      "Iteration 23, loss = 0.02260857\n",
      "Iteration 24, loss = 0.01956410\n",
      "Iteration 25, loss = 0.02109690\n",
      "Iteration 26, loss = 0.01957533\n",
      "Iteration 27, loss = 0.01859789\n",
      "Iteration 28, loss = 0.01962362\n",
      "Iteration 29, loss = 0.01777558\n",
      "Iteration 30, loss = 0.01714302\n",
      "Iteration 31, loss = 0.01854053\n",
      "Iteration 32, loss = 0.01696052\n",
      "Iteration 33, loss = 0.01609270\n",
      "Iteration 34, loss = 0.01567021\n",
      "Iteration 35, loss = 0.01570384\n",
      "Iteration 36, loss = 0.01469208\n",
      "Iteration 37, loss = 0.01708315\n",
      "Iteration 38, loss = 0.01381722\n",
      "Iteration 39, loss = 0.01565627\n",
      "Iteration 40, loss = 0.01314336\n",
      "Iteration 41, loss = 0.01398816\n",
      "Iteration 42, loss = 0.01503445\n",
      "Iteration 43, loss = 0.01421464\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 20): 0.8233172910611091\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06300121666727307\n",
      "MSE (recomputed with last layer only): 0.0630012168399268\n",
      "d=21\n",
      "phi max norm: 162.22\n",
      "theta norm: 0.58\n",
      "MSE (mu): 0.06300121343830967\n",
      "mu: max 6.450007915496826 - min -0.33626115322113037\n",
      "gap max: 6.780401\n",
      "gap min: 7.414818e-05\n",
      "# contexts with gap_min > 0.001: 1319\n",
      "# contexts with gap_min > 0.01: 1290\n",
      "# contexts with gap_min > 0.1: 970\n",
      "Spanning R^11\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 25]\n",
      "Iteration 1, loss = 0.39500096\n",
      "Iteration 2, loss = 0.24569451\n",
      "Iteration 3, loss = 0.14756434\n",
      "Iteration 4, loss = 0.09428278\n",
      "Iteration 5, loss = 0.06953211\n",
      "Iteration 6, loss = 0.05714338\n",
      "Iteration 7, loss = 0.04968775\n",
      "Iteration 8, loss = 0.04475279\n",
      "Iteration 9, loss = 0.04159677\n",
      "Iteration 10, loss = 0.03603028\n",
      "Iteration 11, loss = 0.03663790\n",
      "Iteration 12, loss = 0.03428969\n",
      "Iteration 13, loss = 0.03182039\n",
      "Iteration 14, loss = 0.02834105\n",
      "Iteration 15, loss = 0.02940801\n",
      "Iteration 16, loss = 0.02825617\n",
      "Iteration 17, loss = 0.02782614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.02405765\n",
      "Iteration 19, loss = 0.02527018\n",
      "Iteration 20, loss = 0.02300648\n",
      "Iteration 21, loss = 0.02253428\n",
      "Iteration 22, loss = 0.02313137\n",
      "Iteration 23, loss = 0.02206967\n",
      "Iteration 24, loss = 0.02101728\n",
      "Iteration 25, loss = 0.02040775\n",
      "Iteration 26, loss = 0.02341354\n",
      "Iteration 27, loss = 0.02080733\n",
      "Iteration 28, loss = 0.02014992\n",
      "Iteration 29, loss = 0.01763650\n",
      "Iteration 30, loss = 0.01778642\n",
      "Iteration 31, loss = 0.01646153\n",
      "Iteration 32, loss = 0.01743751\n",
      "Iteration 33, loss = 0.01680756\n",
      "Iteration 34, loss = 0.01524102\n",
      "Iteration 35, loss = 0.01529493\n",
      "Iteration 36, loss = 0.01655751\n",
      "Iteration 37, loss = 0.01419050\n",
      "Iteration 38, loss = 0.01468021\n",
      "Iteration 39, loss = 0.01445894\n",
      "Iteration 40, loss = 0.01423914\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 25): 0.8235541553622471\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06496310176660305\n",
      "MSE (recomputed with last layer only): 0.06496310168878285\n",
      "d=26\n",
      "phi max norm: 128.47\n",
      "theta norm: 0.62\n",
      "MSE (mu): 0.06496310194155097\n",
      "mu: max 6.665828227996826 - min -0.47509801387786865\n",
      "gap max: 6.9891567\n",
      "gap min: 0.0007212162\n",
      "# contexts with gap_min > 0.001: 1320\n",
      "# contexts with gap_min > 0.01: 1277\n",
      "# contexts with gap_min > 0.1: 1007\n",
      "Spanning R^16\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 30]\n",
      "Iteration 1, loss = 0.39330353\n",
      "Iteration 2, loss = 0.24149032\n",
      "Iteration 3, loss = 0.14435530\n",
      "Iteration 4, loss = 0.09469886\n",
      "Iteration 5, loss = 0.07093433\n",
      "Iteration 6, loss = 0.05758704\n",
      "Iteration 7, loss = 0.05039502\n",
      "Iteration 8, loss = 0.04425208\n",
      "Iteration 9, loss = 0.04241184\n",
      "Iteration 10, loss = 0.03810424\n",
      "Iteration 11, loss = 0.03616863\n",
      "Iteration 12, loss = 0.03174208\n",
      "Iteration 13, loss = 0.03393126\n",
      "Iteration 14, loss = 0.03033303\n",
      "Iteration 15, loss = 0.02835054\n",
      "Iteration 16, loss = 0.02862923\n",
      "Iteration 17, loss = 0.02517180\n",
      "Iteration 18, loss = 0.02497081\n",
      "Iteration 19, loss = 0.02469124\n",
      "Iteration 20, loss = 0.02305418\n",
      "Iteration 21, loss = 0.02277759\n",
      "Iteration 22, loss = 0.02023229\n",
      "Iteration 23, loss = 0.02081644\n",
      "Iteration 24, loss = 0.01944178\n",
      "Iteration 25, loss = 0.02028650\n",
      "Iteration 26, loss = 0.01865024\n",
      "Iteration 27, loss = 0.02003515\n",
      "Iteration 28, loss = 0.01758086\n",
      "Iteration 29, loss = 0.01655566\n",
      "Iteration 30, loss = 0.01808200\n",
      "Iteration 31, loss = 0.01730659\n",
      "Iteration 32, loss = 0.01466725\n",
      "Iteration 33, loss = 0.01563747\n",
      "Iteration 34, loss = 0.01558326\n",
      "Iteration 35, loss = 0.01551215\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 30): 0.8190539670742223\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06588723214919227\n",
      "MSE (recomputed with last layer only): 0.06588723205233099\n",
      "d=31\n",
      "phi max norm: 121.33\n",
      "theta norm: 0.68\n",
      "MSE (mu): 0.06588723487463251\n",
      "mu: max 6.828125476837158 - min -0.4098307490348816\n",
      "gap max: 7.1595564\n",
      "gap min: 0.00039696693\n",
      "# contexts with gap_min > 0.001: 1317\n",
      "# contexts with gap_min > 0.01: 1299\n",
      "# contexts with gap_min > 0.1: 1040\n",
      "Spanning R^21\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 35]\n",
      "Iteration 1, loss = 0.39731533\n",
      "Iteration 2, loss = 0.24487944\n",
      "Iteration 3, loss = 0.14683480\n",
      "Iteration 4, loss = 0.09394994\n",
      "Iteration 5, loss = 0.06907171\n",
      "Iteration 6, loss = 0.05897911\n",
      "Iteration 7, loss = 0.04885010\n",
      "Iteration 8, loss = 0.04361828\n",
      "Iteration 9, loss = 0.04033626\n",
      "Iteration 10, loss = 0.03679909\n",
      "Iteration 11, loss = 0.03544167\n",
      "Iteration 12, loss = 0.03272367\n",
      "Iteration 13, loss = 0.03130372\n",
      "Iteration 14, loss = 0.02796171\n",
      "Iteration 15, loss = 0.02919756\n",
      "Iteration 16, loss = 0.02541901\n",
      "Iteration 17, loss = 0.02492964\n",
      "Iteration 18, loss = 0.02435983\n",
      "Iteration 19, loss = 0.02309335\n",
      "Iteration 20, loss = 0.02256204\n",
      "Iteration 21, loss = 0.02155534\n",
      "Iteration 22, loss = 0.02178770\n",
      "Iteration 23, loss = 0.02027101\n",
      "Iteration 24, loss = 0.01850482\n",
      "Iteration 25, loss = 0.01749245\n",
      "Iteration 26, loss = 0.01777600\n",
      "Iteration 27, loss = 0.01817560\n",
      "Iteration 28, loss = 0.01693990\n",
      "Iteration 29, loss = 0.01605395\n",
      "Iteration 30, loss = 0.01620005\n",
      "Iteration 31, loss = 0.01642925\n",
      "Iteration 32, loss = 0.01505709\n",
      "Iteration 33, loss = 0.01600821\n",
      "Iteration 34, loss = 0.01511896\n",
      "Iteration 35, loss = 0.01400010\n",
      "Iteration 36, loss = 0.01346871\n",
      "Iteration 37, loss = 0.01508824\n",
      "Iteration 38, loss = 0.01297661\n",
      "Iteration 39, loss = 0.01377483\n",
      "Iteration 40, loss = 0.01127326\n",
      "Iteration 41, loss = 0.01350287\n",
      "Iteration 42, loss = 0.01329959\n",
      "Iteration 43, loss = 0.01288525\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 35): 0.8506565614190273\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.05020914564645896\n",
      "MSE (recomputed with last layer only): 0.05020914564897613\n",
      "d=36\n",
      "phi max norm: 136.95\n",
      "theta norm: 0.43\n",
      "MSE (mu): 0.050209146514365526\n",
      "mu: max 6.816359519958496 - min -0.4898602068424225\n",
      "gap max: 7.178064\n",
      "gap min: 0.00016403198\n",
      "# contexts with gap_min > 0.001: 1317\n",
      "# contexts with gap_min > 0.01: 1290\n",
      "# contexts with gap_min > 0.1: 1031\n",
      "Spanning R^24\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 40]\n",
      "Iteration 1, loss = 0.39593275\n",
      "Iteration 2, loss = 0.25174447\n",
      "Iteration 3, loss = 0.15279441\n",
      "Iteration 4, loss = 0.09763235\n",
      "Iteration 5, loss = 0.07116611\n",
      "Iteration 6, loss = 0.05862448\n",
      "Iteration 7, loss = 0.05117105\n",
      "Iteration 8, loss = 0.04471762\n",
      "Iteration 9, loss = 0.04031627\n",
      "Iteration 10, loss = 0.03625381\n",
      "Iteration 11, loss = 0.03354327\n",
      "Iteration 12, loss = 0.03397901\n",
      "Iteration 13, loss = 0.03007877\n",
      "Iteration 14, loss = 0.03134610\n",
      "Iteration 15, loss = 0.02617800\n",
      "Iteration 16, loss = 0.02694431\n",
      "Iteration 17, loss = 0.02563433\n",
      "Iteration 18, loss = 0.02411543\n",
      "Iteration 19, loss = 0.02320842\n",
      "Iteration 20, loss = 0.02150060\n",
      "Iteration 21, loss = 0.02028341\n",
      "Iteration 22, loss = 0.02032288\n",
      "Iteration 23, loss = 0.02126736\n",
      "Iteration 24, loss = 0.01996611\n",
      "Iteration 25, loss = 0.01747294\n",
      "Iteration 26, loss = 0.01922111\n",
      "Iteration 27, loss = 0.01614306\n",
      "Iteration 28, loss = 0.01757826\n",
      "Iteration 29, loss = 0.01711787\n",
      "Iteration 30, loss = 0.01489731\n",
      "Iteration 31, loss = 0.01561165\n",
      "Iteration 32, loss = 0.01668397\n",
      "Iteration 33, loss = 0.01331597\n",
      "Iteration 34, loss = 0.01579734\n",
      "Iteration 35, loss = 0.01417722\n",
      "Iteration 36, loss = 0.01388979\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 40): 0.8347174909418704\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.05806022698920362\n",
      "MSE (recomputed with last layer only): 0.05806022697880187\n",
      "d=41\n",
      "phi max norm: 107.84\n",
      "theta norm: 0.64\n",
      "MSE (mu): 0.05806022703901834\n",
      "mu: max 6.678577899932861 - min -0.49195152521133423\n",
      "gap max: 7.0018773\n",
      "gap min: 3.1232834e-05\n",
      "# contexts with gap_min > 0.001: 1318\n",
      "# contexts with gap_min > 0.01: 1290\n",
      "# contexts with gap_min > 0.1: 1020\n",
      "Spanning R^25\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 45]\n",
      "Iteration 1, loss = 0.39440856\n",
      "Iteration 2, loss = 0.24585949\n",
      "Iteration 3, loss = 0.15398275\n",
      "Iteration 4, loss = 0.10114188\n",
      "Iteration 5, loss = 0.07444460\n",
      "Iteration 6, loss = 0.05913350\n",
      "Iteration 7, loss = 0.05184881\n",
      "Iteration 8, loss = 0.04704424\n",
      "Iteration 9, loss = 0.03965694\n",
      "Iteration 10, loss = 0.03927058\n",
      "Iteration 11, loss = 0.03494144\n",
      "Iteration 12, loss = 0.03378783\n",
      "Iteration 13, loss = 0.03227546\n",
      "Iteration 14, loss = 0.02963540\n",
      "Iteration 15, loss = 0.02799004\n",
      "Iteration 16, loss = 0.02612733\n",
      "Iteration 17, loss = 0.02607807\n",
      "Iteration 18, loss = 0.02459105\n",
      "Iteration 19, loss = 0.02278263\n",
      "Iteration 20, loss = 0.02312223\n",
      "Iteration 21, loss = 0.02295246\n",
      "Iteration 22, loss = 0.02082202\n",
      "Iteration 23, loss = 0.02117428\n",
      "Iteration 24, loss = 0.02140894\n",
      "Iteration 25, loss = 0.01881709\n",
      "Iteration 26, loss = 0.01866548\n",
      "Iteration 27, loss = 0.01811210\n",
      "Iteration 28, loss = 0.01818128\n",
      "Iteration 29, loss = 0.01689779\n",
      "Iteration 30, loss = 0.01786069\n",
      "Iteration 31, loss = 0.01542223\n",
      "Iteration 32, loss = 0.01501324\n",
      "Iteration 33, loss = 0.01738374\n",
      "Iteration 34, loss = 0.01478329\n",
      "Iteration 35, loss = 0.01528421\n",
      "Iteration 36, loss = 0.01473387\n",
      "Iteration 37, loss = 0.01495618\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 45): 0.8360892018169895\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.05691209388254116\n",
      "MSE (recomputed with last layer only): 0.05691209391693377\n",
      "d=46\n",
      "phi max norm: 134.18\n",
      "theta norm: 0.71\n",
      "MSE (mu): 0.05691209427640016\n",
      "mu: max 7.040672302246094 - min -0.3575074076652527\n",
      "gap max: 7.3646517\n",
      "gap min: 0.00067043304\n",
      "# contexts with gap_min > 0.001: 1321\n",
      "# contexts with gap_min > 0.01: 1296\n",
      "# contexts with gap_min > 0.1: 1034\n",
      "Spanning R^29\n",
      "lambda HLS: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training NN -- Size [256, 256, 50]\n",
      "Iteration 1, loss = 0.38773072\n",
      "Iteration 2, loss = 0.23882627\n",
      "Iteration 3, loss = 0.14923637\n",
      "Iteration 4, loss = 0.10104521\n",
      "Iteration 5, loss = 0.07385585\n",
      "Iteration 6, loss = 0.06094225\n",
      "Iteration 7, loss = 0.05194645\n",
      "Iteration 8, loss = 0.04586140\n",
      "Iteration 9, loss = 0.04065352\n",
      "Iteration 10, loss = 0.03968504\n",
      "Iteration 11, loss = 0.03607915\n",
      "Iteration 12, loss = 0.03334798\n",
      "Iteration 13, loss = 0.03318276\n",
      "Iteration 14, loss = 0.02916370\n",
      "Iteration 15, loss = 0.02771345\n",
      "Iteration 16, loss = 0.02649797\n",
      "Iteration 17, loss = 0.02579277\n",
      "Iteration 18, loss = 0.02649893\n",
      "Iteration 19, loss = 0.02331741\n",
      "Iteration 20, loss = 0.02212807\n",
      "Iteration 21, loss = 0.02266475\n",
      "Iteration 22, loss = 0.02143217\n",
      "Iteration 23, loss = 0.02157554\n",
      "Iteration 24, loss = 0.01946696\n",
      "Iteration 25, loss = 0.02072789\n",
      "Iteration 26, loss = 0.01823458\n",
      "Iteration 27, loss = 0.01857059\n",
      "Iteration 28, loss = 0.01737206\n",
      "Iteration 29, loss = 0.01763318\n",
      "Iteration 30, loss = 0.01838678\n",
      "Iteration 31, loss = 0.01591185\n",
      "Iteration 32, loss = 0.01665422\n",
      "Iteration 33, loss = 0.01579941\n",
      "Iteration 34, loss = 0.01475509\n",
      "Iteration 35, loss = 0.01531814\n",
      "Iteration 36, loss = 0.01432828\n",
      "Iteration 37, loss = 0.01389288\n",
      "Iteration 38, loss = 0.01536115\n",
      "Iteration 39, loss = 0.01453694\n",
      "Iteration 40, loss = 0.01235043\n",
      "Iteration 41, loss = 0.01324864\n",
      "Iteration 42, loss = 0.01352036\n",
      "Iteration 43, loss = 0.01235858\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 50): 0.8251878408408849\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.060536054568688186\n",
      "MSE (recomputed with last layer only): 0.0605360547385835\n",
      "d=51\n",
      "phi max norm: 155.54\n",
      "theta norm: 0.72\n",
      "MSE (mu): 0.06053605495757506\n",
      "mu: max 7.632641792297363 - min -0.509198784828186\n",
      "gap max: 7.955669\n",
      "gap min: 0.00044202805\n",
      "# contexts with gap_min > 0.001: 1316\n",
      "# contexts with gap_min > 0.01: 1273\n",
      "# contexts with gap_min > 0.1: 1010\n",
      "Spanning R^33\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 55]\n",
      "Iteration 1, loss = 0.39062557\n",
      "Iteration 2, loss = 0.24073225\n",
      "Iteration 3, loss = 0.14459087\n",
      "Iteration 4, loss = 0.09209372\n",
      "Iteration 5, loss = 0.06951321\n",
      "Iteration 6, loss = 0.05730346\n",
      "Iteration 7, loss = 0.04823115\n",
      "Iteration 8, loss = 0.04552315\n",
      "Iteration 9, loss = 0.03952567\n",
      "Iteration 10, loss = 0.03666163\n",
      "Iteration 11, loss = 0.03337035\n",
      "Iteration 12, loss = 0.03492033\n",
      "Iteration 13, loss = 0.03061725\n",
      "Iteration 14, loss = 0.02919773\n",
      "Iteration 15, loss = 0.02911221\n",
      "Iteration 16, loss = 0.02769318\n",
      "Iteration 17, loss = 0.02445168\n",
      "Iteration 18, loss = 0.02397217\n",
      "Iteration 19, loss = 0.02526151\n",
      "Iteration 20, loss = 0.02228465\n",
      "Iteration 21, loss = 0.02146349\n",
      "Iteration 22, loss = 0.02168035\n",
      "Iteration 23, loss = 0.01913072\n",
      "Iteration 24, loss = 0.02108444\n",
      "Iteration 25, loss = 0.02079858\n",
      "Iteration 26, loss = 0.01807173\n",
      "Iteration 27, loss = 0.01817413\n",
      "Iteration 28, loss = 0.01758047\n",
      "Iteration 29, loss = 0.01674594\n",
      "Iteration 30, loss = 0.01777765\n",
      "Iteration 31, loss = 0.01566412\n",
      "Iteration 32, loss = 0.01525795\n",
      "Iteration 33, loss = 0.01724630\n",
      "Iteration 34, loss = 0.01466159\n",
      "Iteration 35, loss = 0.01476428\n",
      "Iteration 36, loss = 0.01490033\n",
      "Iteration 37, loss = 0.01505036\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 55): 0.8120378516365655\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.06954942603169152\n",
      "MSE (recomputed with last layer only): 0.06954942606324276\n",
      "d=56\n",
      "phi max norm: 95.64\n",
      "theta norm: 0.67\n",
      "MSE (mu): 0.06954942500377949\n",
      "mu: max 7.867968559265137 - min -0.653701901435852\n",
      "gap max: 8.234941\n",
      "gap min: 5.722046e-06\n",
      "# contexts with gap_min > 0.001: 1316\n",
      "# contexts with gap_min > 0.01: 1284\n",
      "# contexts with gap_min > 0.1: 1000\n",
      "Spanning R^46\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 256, 60]\n",
      "Iteration 1, loss = 0.38696942\n",
      "Iteration 2, loss = 0.23275750\n",
      "Iteration 3, loss = 0.14257685\n",
      "Iteration 4, loss = 0.09501243\n",
      "Iteration 5, loss = 0.07412697\n",
      "Iteration 6, loss = 0.06168133\n",
      "Iteration 7, loss = 0.05267746\n",
      "Iteration 8, loss = 0.04701694\n",
      "Iteration 9, loss = 0.04303924\n",
      "Iteration 10, loss = 0.03768836\n",
      "Iteration 11, loss = 0.03788168\n",
      "Iteration 12, loss = 0.03447638\n",
      "Iteration 13, loss = 0.03148460\n",
      "Iteration 14, loss = 0.03241954\n",
      "Iteration 15, loss = 0.02875994\n",
      "Iteration 16, loss = 0.02670608\n",
      "Iteration 17, loss = 0.02574893\n",
      "Iteration 18, loss = 0.02527613\n",
      "Iteration 19, loss = 0.02432868\n",
      "Iteration 20, loss = 0.02371744\n",
      "Iteration 21, loss = 0.02172731\n",
      "Iteration 22, loss = 0.02187752\n",
      "Iteration 23, loss = 0.02149792\n",
      "Iteration 24, loss = 0.01942100\n",
      "Iteration 25, loss = 0.02011537\n",
      "Iteration 26, loss = 0.01931381\n",
      "Iteration 27, loss = 0.01893907\n",
      "Iteration 28, loss = 0.01723883\n",
      "Iteration 29, loss = 0.01891779\n",
      "Iteration 30, loss = 0.01644678\n",
      "Iteration 31, loss = 0.01771917\n",
      "Iteration 32, loss = 0.01540856\n",
      "Iteration 33, loss = 0.01637375\n",
      "Iteration 34, loss = 0.01530212\n",
      "Iteration 35, loss = 0.01500081\n",
      "Iteration 36, loss = 0.01498697\n",
      "Iteration 37, loss = 0.01484231\n",
      "Iteration 38, loss = 0.01350841\n",
      "Iteration 39, loss = 0.01453497\n",
      "Iteration 40, loss = 0.01459315\n",
      "Iteration 41, loss = 0.01419360\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2 (size 60): 0.8309468904181856\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.057590626625169075\n",
      "MSE (recomputed with last layer only): 0.057590626698158745\n",
      "d=61\n",
      "phi max norm: 117.71\n",
      "theta norm: 0.69\n",
      "MSE (mu): 0.057590626859879986\n",
      "mu: max 7.2067036628723145 - min -0.5329912900924683\n",
      "gap max: 7.569202\n",
      "gap min: 0.00020551682\n",
      "# contexts with gap_min > 0.001: 1319\n",
      "# contexts with gap_min > 0.01: 1289\n",
      "# contexts with gap_min > 0.1: 1013\n",
      "Spanning R^35\n",
      "lambda HLS: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit networks\n",
    "\n",
    "hidden = [256, 256]\n",
    "ds = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "test_size=0.25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "for j in ds:\n",
    "    size = hidden + [j]\n",
    "    print(\"Training NN -- Size {0}\".format(size))\n",
    "    net = MLPRegressor(hidden_layer_sizes=size, max_iter=500, verbose=True).fit(X_train, y_train)\n",
    "    print(\"R^2 (size {0}): {1}\".format(j, net.score(X_test, y_test)))\n",
    "    print()\n",
    "    print(\"Saving model...\")\n",
    "    save_model(net)\n",
    "    del(net)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
