{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spanrd(vectors, d):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - vectors (array): matrix (N, d)\n",
    "        - d (int): dimension of the space to be spanned\n",
    "    Return:\n",
    "        - True or False\n",
    "    \"\"\"\n",
    "    # https://math.stackexchange.com/questions/56201/how-to-tell-if-a-set-of-vectors-spans-a-space\n",
    "    # https://stackoverflow.com/questions/15638650/is-there-a-standard-solution-for-gauss-elimination-in-python\n",
    "    pl, u = lu(vectors, permute_l=True)\n",
    "    rank = np.linalg.matrix_rank(u)\n",
    "    return d == int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (19181, 40)\n",
      "ratings: max 0.9710000000000001 - min -0.9949999999999999\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data_path = \"jester_data_40jokes_19181users.npy\"\n",
    "\n",
    "ratings = np.load(data_path)\n",
    "print(\"Loaded dataset: {}\".format(ratings.shape))\n",
    "\n",
    "n_users, n_items = ratings.shape\n",
    "ratings = ratings / 10  # normalize ratings\n",
    "print(\"ratings: max {0} - min {1}\".format(ratings.max(), ratings.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  36\n",
      "RMSE: 0.09934649260590526\n",
      "MAX_ERR: 1.4339307776635835\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "\n",
    "K = 36\n",
    "U, s, Vt = svds(ratings, k=K)\n",
    "s = np.diag(s)\n",
    "U = np.dot(U, s)\n",
    "\n",
    "# MSE\n",
    "Yhat = U.dot(Vt)\n",
    "rmse = np.sqrt(np.mean(np.abs(Yhat - ratings) ** 2))\n",
    "print(\"K: \", K)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAX_ERR:\", np.abs(Yhat - ratings).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(net, normalize=False):\n",
    "\n",
    "    # Build features\n",
    "    X_pred = X\n",
    "\n",
    "    hidden_layer_sizes = list(net.hidden_layer_sizes)\n",
    "\n",
    "    layer_units = [X_pred.shape[1]] + hidden_layer_sizes + [1]\n",
    "    activations = [X_pred]\n",
    "    for i in range(net.n_layers_ - 1):\n",
    "        activations.append(np.empty((X_pred.shape[0], layer_units[i + 1])))\n",
    "\n",
    "    net._forward_pass(activations)\n",
    "    y_pred = activations[-1]\n",
    "    print(\"MSE (original):\", np.mean((y_pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get weights\n",
    "    last_w = net.coefs_[-1]\n",
    "    bias = np.array(net.intercepts_[-1]).reshape((1, 1))\n",
    "    last_w = np.concatenate([last_w, bias])\n",
    "\n",
    "    # get last-layer features\n",
    "    last_feat = np.array(activations[-2], dtype=np.float32)\n",
    "    last_feat = np.concatenate([last_feat, np.ones((X_pred.shape[0], 1))], axis=1)\n",
    "\n",
    "    # get prediction\n",
    "    pred = last_feat.dot(last_w)\n",
    "    print(\"MSE (recomputed with last layer only):\", np.mean((pred.flatten() - y) ** 2))\n",
    "\n",
    "    # get feature matrix\n",
    "    d = hidden_layer_sizes[-1] + 1\n",
    "    print(\"d={0}\".format(d))\n",
    "    phi = np.empty((n_users, n_items, d), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for t in range(n_users):\n",
    "        for z in range(n_items):\n",
    "            phi[t, z, :] = last_feat[idx, :] / (np.linalg.norm(last_feat[idx, :]) if normalize else 1)\n",
    "            idx += 1\n",
    "    assert idx == last_feat.shape[0]\n",
    "\n",
    "    # get param\n",
    "    theta = np.array(last_w, dtype=np.float32).squeeze()\n",
    "    if normalize:\n",
    "        theta = theta / np.linalg.norm(theta)\n",
    "        \n",
    "    phi_norm = round(np.linalg.norm(phi, axis=2).max(), 2)\n",
    "    print(\"phi max norm:\", phi_norm)\n",
    "    theta_norm = round(np.linalg.norm(theta), 2)\n",
    "    print(\"theta norm:\", theta_norm)\n",
    "\n",
    "    # check predictions\n",
    "    mu = phi.dot(theta)\n",
    "    print(\"MSE (mu):\", np.mean(np.abs(ratings - mu).flatten()**2))\n",
    "    print(\"mu: max {0} - min {1}\".format(mu.max(), mu.min()))\n",
    "    gap = np.max(mu, axis=1)[:, np.newaxis] - mu\n",
    "    print(\"gap max:\", gap.max())\n",
    "    gap[gap == 0] = 100\n",
    "    print(\"gap min:\", gap.min())\n",
    "    gap = np.min(gap, axis=1)\n",
    "    print(\"# contexts with gap_min > 0.001:\", np.sum(gap > 0.001))\n",
    "    print(\"# contexts with gap_min > 0.01:\", np.sum(gap > 0.01))\n",
    "    print(\"# contexts with gap_min > 0.1:\", np.sum(gap > 0.1))\n",
    "\n",
    "    # check span\n",
    "    astar = np.argmax(mu, axis=1)\n",
    "    fstar = np.array([phi[x, astar[x]] for x in range(n_users)])\n",
    "\n",
    "    span = d\n",
    "    for i in range(d):\n",
    "        if check_spanrd(fstar, d - i):\n",
    "            span = d - i\n",
    "            break\n",
    "\n",
    "    print(\"{0}Spanning R^{1}\".format(\"WARNING: \" if span == d else \"\", span))\n",
    "    \n",
    "    # compute lambda HLS\n",
    "    \n",
    "    outer = np.matmul(fstar.T, fstar) / n_users\n",
    "    lambda_hls = np.linalg.eigvals(outer).min()\n",
    "    print(\"lambda HLS:\", lambda_hls)\n",
    "\n",
    "    # save\n",
    "    np.savez_compressed('jester_d{0}_span{1}_L{2:.2f}_S{3:.2f}_hls{4:.5f}.npz'.format(d,span,phi_norm,theta_norm, lambda_hls), \n",
    "                        features=phi, theta=theta)\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN -- Size (32, 32)\n",
      "Iteration 1, loss = 0.06032827\n",
      "Iteration 2, loss = 0.02434696\n",
      "Iteration 3, loss = 0.01662422\n",
      "Iteration 4, loss = 0.01492757\n",
      "Iteration 5, loss = 0.01425305\n",
      "Iteration 6, loss = 0.01388699\n",
      "Iteration 7, loss = 0.01364689\n",
      "Iteration 8, loss = 0.01348789\n",
      "Iteration 9, loss = 0.01337995\n",
      "Iteration 10, loss = 0.01323149\n",
      "Iteration 11, loss = 0.01312550\n",
      "Iteration 12, loss = 0.01306423\n",
      "Iteration 13, loss = 0.01299734\n",
      "Iteration 14, loss = 0.01290144\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "R^2: 0.900135596093936\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.025317196528455283\n",
      "MSE (recomputed with last layer only): 0.025317196520806905\n",
      "d=33\n",
      "phi max norm: 4.95\n",
      "theta norm: 4.09\n",
      "MSE (mu): 0.02531719568480024\n",
      "mu: max 1.5421335697174072 - min -1.9387906789779663\n",
      "gap max: 3.3288007\n",
      "gap min: 4.172325e-06\n",
      "# contexts with gap_min > 0.001: 18959\n",
      "# contexts with gap_min > 0.01: 17037\n",
      "# contexts with gap_min > 0.1: 5437\n",
      "WARNING: Spanning R^33\n",
      "lambda HLS: 0.001856959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit large \"ground-truth\" network\n",
    "\n",
    "hidden = 32\n",
    "test_size=0.25\n",
    "\n",
    "X, y = [], []\n",
    "for t in range(n_users):\n",
    "    for z in range(n_items):\n",
    "        feat = np.concatenate([U[t], Vt[:, z]]).ravel()\n",
    "        X.append(feat)\n",
    "        y.append(ratings[t, z])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "print(\"Training NN -- Size {0}\".format((hidden, hidden)))\n",
    "net = MLPRegressor(hidden_layer_sizes=(hidden, hidden), max_iter=500, verbose=True).fit(X_train, y_train)\n",
    "print(\"R^2:\", net.score(X_test, y_test))\n",
    "print()\n",
    "print(\"Saving model...\")\n",
    "mu = save_model(net)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN -- Size [256, 30]\n",
      "Iteration 1, loss = 0.01249646\n",
      "Iteration 2, loss = 0.00249061\n",
      "Iteration 3, loss = 0.00179495\n",
      "Iteration 4, loss = 0.00150755\n",
      "Iteration 5, loss = 0.00135836\n",
      "Iteration 6, loss = 0.00126623\n",
      "Iteration 7, loss = 0.00120708\n",
      "Iteration 8, loss = 0.00116342\n",
      "Iteration 9, loss = 0.00112710\n",
      "Iteration 10, loss = 0.00109337\n",
      "Iteration 11, loss = 0.00107069\n",
      "Iteration 12, loss = 0.00104686\n",
      "Iteration 13, loss = 0.00102835\n",
      "Iteration 14, loss = 0.00101479\n",
      "Iteration 15, loss = 0.00099417\n",
      "Iteration 16, loss = 0.00097794\n",
      "Iteration 17, loss = 0.00096156\n",
      "Iteration 18, loss = 0.00094907\n",
      "Iteration 19, loss = 0.00093359\n",
      "Iteration 20, loss = 0.00092460\n",
      "Iteration 21, loss = 0.00091588\n",
      "Iteration 22, loss = 0.00089937\n",
      "Iteration 23, loss = 0.00088933\n",
      "Iteration 24, loss = 0.00088377\n",
      "Iteration 25, loss = 0.00087046\n",
      "Iteration 26, loss = 0.00086728\n",
      "Iteration 27, loss = 0.00085086\n",
      "Iteration 28, loss = 0.00084028\n",
      "Iteration 29, loss = 0.00083053\n",
      "Iteration 30, loss = 0.00082609\n",
      "Iteration 31, loss = 0.00081462\n",
      "Iteration 32, loss = 0.00080811\n",
      "Iteration 33, loss = 0.00079049\n",
      "Iteration 34, loss = 0.00079015\n",
      "Iteration 35, loss = 0.00078368\n",
      "Iteration 36, loss = 0.00077631\n",
      "Iteration 37, loss = 0.00076814\n",
      "Iteration 38, loss = 0.00076530\n",
      "Iteration 39, loss = 0.00075666\n",
      "Iteration 40, loss = 0.00075085\n",
      "Iteration 41, loss = 0.00074293\n",
      "Iteration 42, loss = 0.00073475\n",
      "Iteration 43, loss = 0.00073304\n",
      "Iteration 44, loss = 0.00072395\n",
      "Iteration 45, loss = 0.00071965\n",
      "Iteration 46, loss = 0.00071155\n",
      "Iteration 47, loss = 0.00070791\n",
      "Iteration 48, loss = 0.00070222\n",
      "Iteration 49, loss = 0.00069818\n",
      "Iteration 50, loss = 0.00069124\n",
      "Iteration 51, loss = 0.00068400\n",
      "Iteration 52, loss = 0.00068286\n",
      "Iteration 53, loss = 0.00067575\n",
      "Iteration 54, loss = 0.00067234\n",
      "Iteration 55, loss = 0.00066561\n",
      "Iteration 56, loss = 0.00066298\n",
      "Iteration 57, loss = 0.00065912\n",
      "Iteration 58, loss = 0.00065450\n",
      "Iteration 59, loss = 0.00064592\n",
      "Iteration 60, loss = 0.00064612\n",
      "Iteration 61, loss = 0.00064288\n",
      "Iteration 62, loss = 0.00063956\n",
      "Iteration 63, loss = 0.00063384\n",
      "Iteration 64, loss = 0.00062921\n",
      "Iteration 65, loss = 0.00062656\n",
      "Iteration 66, loss = 0.00062734\n",
      "Iteration 67, loss = 0.00062307\n",
      "Iteration 68, loss = 0.00061849\n",
      "Iteration 69, loss = 0.00061563\n",
      "Iteration 70, loss = 0.00061087\n",
      "Iteration 71, loss = 0.00061185\n",
      "Iteration 72, loss = 0.00060605\n",
      "Iteration 73, loss = 0.00060452\n",
      "Iteration 74, loss = 0.00060344\n",
      "Iteration 75, loss = 0.00060204\n",
      "Iteration 76, loss = 0.00059906\n",
      "Iteration 77, loss = 0.00059506\n",
      "Iteration 78, loss = 0.00059328\n",
      "Iteration 79, loss = 0.00059210\n",
      "Iteration 80, loss = 0.00058957\n",
      "Iteration 81, loss = 0.00059170\n",
      "Iteration 82, loss = 0.00058475\n",
      "Iteration 83, loss = 0.00058539\n",
      "Iteration 84, loss = 0.00058154\n",
      "Iteration 85, loss = 0.00058079\n",
      "Iteration 86, loss = 0.00057803\n",
      "Iteration 87, loss = 0.00057778\n",
      "Iteration 88, loss = 0.00057635\n",
      "Iteration 89, loss = 0.00057294\n",
      "Iteration 90, loss = 0.00057047\n",
      "Iteration 91, loss = 0.00057047\n",
      "Iteration 92, loss = 0.00056910\n",
      "Iteration 93, loss = 0.00056676\n",
      "Iteration 94, loss = 0.00056715\n",
      "Iteration 95, loss = 0.00056535\n",
      "Iteration 96, loss = 0.00056160\n",
      "Iteration 97, loss = 0.00056506\n",
      "Iteration 98, loss = 0.00056250\n",
      "Iteration 99, loss = 0.00056013\n",
      "Iteration 100, loss = 0.00055971\n",
      "Iteration 101, loss = 0.00056150\n",
      "Iteration 102, loss = 0.00055818\n",
      "Iteration 103, loss = 0.00055523\n",
      "Iteration 104, loss = 0.00055920\n",
      "Iteration 105, loss = 0.00055416\n",
      "Iteration 106, loss = 0.00055587\n",
      "Iteration 107, loss = 0.00055407\n",
      "Iteration 108, loss = 0.00055590\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 30): 0.9970232109629962\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.0249991137862115\n",
      "MSE (recomputed with last layer only): 0.024999113786552177\n",
      "d=31\n",
      "phi max norm: 4.13\n",
      "theta norm: 1.85\n",
      "MSE (mu): 0.024999113479760186\n",
      "mu: max 1.4966599941253662 - min -1.7338688373565674\n",
      "gap max: 2.721211\n",
      "gap min: 3.695488e-06\n",
      "# contexts with gap_min > 0.001: 18948\n",
      "# contexts with gap_min > 0.01: 16915\n",
      "# contexts with gap_min > 0.1: 5172\n",
      "Spanning R^23\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 28]\n",
      "Iteration 1, loss = 0.01215252\n",
      "Iteration 2, loss = 0.00241212\n",
      "Iteration 3, loss = 0.00171228\n",
      "Iteration 4, loss = 0.00144838\n",
      "Iteration 5, loss = 0.00131252\n",
      "Iteration 6, loss = 0.00122861\n",
      "Iteration 7, loss = 0.00117006\n",
      "Iteration 8, loss = 0.00112784\n",
      "Iteration 9, loss = 0.00109012\n",
      "Iteration 10, loss = 0.00105946\n",
      "Iteration 11, loss = 0.00102700\n",
      "Iteration 12, loss = 0.00100317\n",
      "Iteration 13, loss = 0.00097841\n",
      "Iteration 14, loss = 0.00096254\n",
      "Iteration 15, loss = 0.00093922\n",
      "Iteration 16, loss = 0.00092668\n",
      "Iteration 17, loss = 0.00091177\n",
      "Iteration 18, loss = 0.00089412\n",
      "Iteration 19, loss = 0.00088046\n",
      "Iteration 20, loss = 0.00086943\n",
      "Iteration 21, loss = 0.00085535\n",
      "Iteration 22, loss = 0.00084360\n",
      "Iteration 23, loss = 0.00083129\n",
      "Iteration 24, loss = 0.00081665\n",
      "Iteration 25, loss = 0.00080551\n",
      "Iteration 26, loss = 0.00079606\n",
      "Iteration 27, loss = 0.00078433\n",
      "Iteration 28, loss = 0.00077333\n",
      "Iteration 29, loss = 0.00076318\n",
      "Iteration 30, loss = 0.00075227\n",
      "Iteration 31, loss = 0.00074726\n",
      "Iteration 32, loss = 0.00073770\n",
      "Iteration 33, loss = 0.00073127\n",
      "Iteration 34, loss = 0.00072245\n",
      "Iteration 35, loss = 0.00071786\n",
      "Iteration 36, loss = 0.00070820\n",
      "Iteration 37, loss = 0.00070273\n",
      "Iteration 38, loss = 0.00069736\n",
      "Iteration 39, loss = 0.00069196\n",
      "Iteration 40, loss = 0.00068272\n",
      "Iteration 41, loss = 0.00068288\n",
      "Iteration 42, loss = 0.00067495\n",
      "Iteration 43, loss = 0.00067288\n",
      "Iteration 44, loss = 0.00066223\n",
      "Iteration 45, loss = 0.00066041\n",
      "Iteration 46, loss = 0.00065777\n",
      "Iteration 47, loss = 0.00065304\n",
      "Iteration 48, loss = 0.00064548\n",
      "Iteration 49, loss = 0.00064187\n",
      "Iteration 50, loss = 0.00063705\n",
      "Iteration 51, loss = 0.00063140\n",
      "Iteration 52, loss = 0.00062941\n",
      "Iteration 53, loss = 0.00062577\n",
      "Iteration 54, loss = 0.00062043\n",
      "Iteration 55, loss = 0.00061815\n",
      "Iteration 56, loss = 0.00061484\n",
      "Iteration 57, loss = 0.00060936\n",
      "Iteration 58, loss = 0.00060895\n",
      "Iteration 59, loss = 0.00060633\n",
      "Iteration 60, loss = 0.00060046\n",
      "Iteration 61, loss = 0.00060207\n",
      "Iteration 62, loss = 0.00059970\n",
      "Iteration 63, loss = 0.00059379\n",
      "Iteration 64, loss = 0.00059103\n",
      "Iteration 65, loss = 0.00058998\n",
      "Iteration 66, loss = 0.00058875\n",
      "Iteration 67, loss = 0.00058311\n",
      "Iteration 68, loss = 0.00058253\n",
      "Iteration 69, loss = 0.00057940\n",
      "Iteration 70, loss = 0.00057660\n",
      "Iteration 71, loss = 0.00057615\n",
      "Iteration 72, loss = 0.00057233\n",
      "Iteration 73, loss = 0.00057292\n",
      "Iteration 74, loss = 0.00057060\n",
      "Iteration 75, loss = 0.00056766\n",
      "Iteration 76, loss = 0.00056586\n",
      "Iteration 77, loss = 0.00056616\n",
      "Iteration 78, loss = 0.00056306\n",
      "Iteration 79, loss = 0.00056172\n",
      "Iteration 80, loss = 0.00056163\n",
      "Iteration 81, loss = 0.00055925\n",
      "Iteration 82, loss = 0.00055670\n",
      "Iteration 83, loss = 0.00055518\n",
      "Iteration 84, loss = 0.00055639\n",
      "Iteration 85, loss = 0.00055518\n",
      "Iteration 86, loss = 0.00055026\n",
      "Iteration 87, loss = 0.00055239\n",
      "Iteration 88, loss = 0.00055042\n",
      "Iteration 89, loss = 0.00054811\n",
      "Iteration 90, loss = 0.00054422\n",
      "Iteration 91, loss = 0.00054651\n",
      "Iteration 92, loss = 0.00054526\n",
      "Iteration 93, loss = 0.00054037\n",
      "Iteration 94, loss = 0.00054078\n",
      "Iteration 95, loss = 0.00054143\n",
      "Iteration 96, loss = 0.00053727\n",
      "Iteration 97, loss = 0.00053754\n",
      "Iteration 98, loss = 0.00053859\n",
      "Iteration 99, loss = 0.00053515\n",
      "Iteration 100, loss = 0.00053558\n",
      "Iteration 101, loss = 0.00053174\n",
      "Iteration 102, loss = 0.00053300\n",
      "Iteration 103, loss = 0.00053201\n",
      "Iteration 104, loss = 0.00053283\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 28): 0.9972260674757514\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.025016583227968243\n",
      "MSE (recomputed with last layer only): 0.025016583226132646\n",
      "d=29\n",
      "phi max norm: 4.08\n",
      "theta norm: 1.96\n",
      "MSE (mu): 0.025016583037970937\n",
      "mu: max 1.5257971286773682 - min -1.6141490936279297\n",
      "gap max: 2.6835456\n",
      "gap min: 3.0994415e-06\n",
      "# contexts with gap_min > 0.001: 18965\n",
      "# contexts with gap_min > 0.01: 17035\n",
      "# contexts with gap_min > 0.1: 5352\n",
      "Spanning R^26\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 26]\n",
      "Iteration 1, loss = 0.01190037\n",
      "Iteration 2, loss = 0.00246045\n",
      "Iteration 3, loss = 0.00173435\n",
      "Iteration 4, loss = 0.00146202\n",
      "Iteration 5, loss = 0.00132906\n",
      "Iteration 6, loss = 0.00124709\n",
      "Iteration 7, loss = 0.00118273\n",
      "Iteration 8, loss = 0.00113964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.00110482\n",
      "Iteration 10, loss = 0.00107546\n",
      "Iteration 11, loss = 0.00105031\n",
      "Iteration 12, loss = 0.00102116\n",
      "Iteration 13, loss = 0.00100598\n",
      "Iteration 14, loss = 0.00098168\n",
      "Iteration 15, loss = 0.00096804\n",
      "Iteration 16, loss = 0.00095368\n",
      "Iteration 17, loss = 0.00093887\n",
      "Iteration 18, loss = 0.00092397\n",
      "Iteration 19, loss = 0.00091062\n",
      "Iteration 20, loss = 0.00089745\n",
      "Iteration 21, loss = 0.00088883\n",
      "Iteration 22, loss = 0.00087691\n",
      "Iteration 23, loss = 0.00086811\n",
      "Iteration 24, loss = 0.00085637\n",
      "Iteration 25, loss = 0.00084583\n",
      "Iteration 26, loss = 0.00083490\n",
      "Iteration 27, loss = 0.00082888\n",
      "Iteration 28, loss = 0.00081765\n",
      "Iteration 29, loss = 0.00081047\n",
      "Iteration 30, loss = 0.00080219\n",
      "Iteration 31, loss = 0.00079515\n",
      "Iteration 32, loss = 0.00078489\n",
      "Iteration 33, loss = 0.00077700\n",
      "Iteration 34, loss = 0.00077046\n",
      "Iteration 35, loss = 0.00076306\n",
      "Iteration 36, loss = 0.00075883\n",
      "Iteration 37, loss = 0.00074536\n",
      "Iteration 38, loss = 0.00074448\n",
      "Iteration 39, loss = 0.00073388\n",
      "Iteration 40, loss = 0.00072911\n",
      "Iteration 41, loss = 0.00072171\n",
      "Iteration 42, loss = 0.00071635\n",
      "Iteration 43, loss = 0.00070928\n",
      "Iteration 44, loss = 0.00070706\n",
      "Iteration 45, loss = 0.00069882\n",
      "Iteration 46, loss = 0.00069391\n",
      "Iteration 47, loss = 0.00068938\n",
      "Iteration 48, loss = 0.00068576\n",
      "Iteration 49, loss = 0.00068022\n",
      "Iteration 50, loss = 0.00067613\n",
      "Iteration 51, loss = 0.00067185\n",
      "Iteration 52, loss = 0.00066645\n",
      "Iteration 53, loss = 0.00066313\n",
      "Iteration 54, loss = 0.00066108\n",
      "Iteration 55, loss = 0.00065799\n",
      "Iteration 56, loss = 0.00065428\n",
      "Iteration 57, loss = 0.00064936\n",
      "Iteration 58, loss = 0.00064976\n",
      "Iteration 59, loss = 0.00064294\n",
      "Iteration 60, loss = 0.00064162\n",
      "Iteration 61, loss = 0.00063547\n",
      "Iteration 62, loss = 0.00063473\n",
      "Iteration 63, loss = 0.00062989\n",
      "Iteration 64, loss = 0.00062824\n",
      "Iteration 65, loss = 0.00062798\n",
      "Iteration 66, loss = 0.00062239\n",
      "Iteration 67, loss = 0.00062161\n",
      "Iteration 68, loss = 0.00061713\n",
      "Iteration 69, loss = 0.00061439\n",
      "Iteration 70, loss = 0.00060916\n",
      "Iteration 71, loss = 0.00060835\n",
      "Iteration 72, loss = 0.00060648\n",
      "Iteration 73, loss = 0.00060670\n",
      "Iteration 74, loss = 0.00060272\n",
      "Iteration 75, loss = 0.00059977\n",
      "Iteration 76, loss = 0.00059896\n",
      "Iteration 77, loss = 0.00059668\n",
      "Iteration 78, loss = 0.00059460\n",
      "Iteration 79, loss = 0.00059379\n",
      "Iteration 80, loss = 0.00059101\n",
      "Iteration 81, loss = 0.00059067\n",
      "Iteration 82, loss = 0.00058709\n",
      "Iteration 83, loss = 0.00058651\n",
      "Iteration 84, loss = 0.00058577\n",
      "Iteration 85, loss = 0.00058415\n",
      "Iteration 86, loss = 0.00058177\n",
      "Iteration 87, loss = 0.00058035\n",
      "Iteration 88, loss = 0.00058060\n",
      "Iteration 89, loss = 0.00057761\n",
      "Iteration 90, loss = 0.00057607\n",
      "Iteration 91, loss = 0.00057265\n",
      "Iteration 92, loss = 0.00057155\n",
      "Iteration 93, loss = 0.00057015\n",
      "Iteration 94, loss = 0.00057024\n",
      "Iteration 95, loss = 0.00056714\n",
      "Iteration 96, loss = 0.00056799\n",
      "Iteration 97, loss = 0.00056477\n",
      "Iteration 98, loss = 0.00056484\n",
      "Iteration 99, loss = 0.00056469\n",
      "Iteration 100, loss = 0.00056260\n",
      "Iteration 101, loss = 0.00056012\n",
      "Iteration 102, loss = 0.00055965\n",
      "Iteration 103, loss = 0.00056278\n",
      "Iteration 104, loss = 0.00055993\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 26): 0.996939352267635\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.02520239206804191\n",
      "MSE (recomputed with last layer only): 0.02520239207423446\n",
      "d=27\n",
      "phi max norm: 3.97\n",
      "theta norm: 2.11\n",
      "MSE (mu): 0.025202391932446184\n",
      "mu: max 1.4556562900543213 - min -1.714043378829956\n",
      "gap max: 2.7256775\n",
      "gap min: 1.7881393e-07\n",
      "# contexts with gap_min > 0.001: 18958\n",
      "# contexts with gap_min > 0.01: 16951\n",
      "# contexts with gap_min > 0.1: 5202\n",
      "Spanning R^24\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 24]\n",
      "Iteration 1, loss = 0.01335092\n",
      "Iteration 2, loss = 0.00250122\n",
      "Iteration 3, loss = 0.00179717\n",
      "Iteration 4, loss = 0.00152155\n",
      "Iteration 5, loss = 0.00136590\n",
      "Iteration 6, loss = 0.00128125\n",
      "Iteration 7, loss = 0.00121593\n",
      "Iteration 8, loss = 0.00117530\n",
      "Iteration 9, loss = 0.00114045\n",
      "Iteration 10, loss = 0.00110539\n",
      "Iteration 11, loss = 0.00108381\n",
      "Iteration 12, loss = 0.00106358\n",
      "Iteration 13, loss = 0.00104346\n",
      "Iteration 14, loss = 0.00103266\n",
      "Iteration 15, loss = 0.00100921\n",
      "Iteration 16, loss = 0.00099695\n",
      "Iteration 17, loss = 0.00098380\n",
      "Iteration 18, loss = 0.00096901\n",
      "Iteration 19, loss = 0.00095506\n",
      "Iteration 20, loss = 0.00094017\n",
      "Iteration 21, loss = 0.00093274\n",
      "Iteration 22, loss = 0.00091526\n",
      "Iteration 23, loss = 0.00090991\n",
      "Iteration 24, loss = 0.00089514\n",
      "Iteration 25, loss = 0.00088732\n",
      "Iteration 26, loss = 0.00087811\n",
      "Iteration 27, loss = 0.00086725\n",
      "Iteration 28, loss = 0.00086098\n",
      "Iteration 29, loss = 0.00085033\n",
      "Iteration 30, loss = 0.00084161\n",
      "Iteration 31, loss = 0.00082898\n",
      "Iteration 32, loss = 0.00082158\n",
      "Iteration 33, loss = 0.00081415\n",
      "Iteration 34, loss = 0.00080375\n",
      "Iteration 35, loss = 0.00079584\n",
      "Iteration 36, loss = 0.00078550\n",
      "Iteration 37, loss = 0.00077913\n",
      "Iteration 38, loss = 0.00077222\n",
      "Iteration 39, loss = 0.00076320\n",
      "Iteration 40, loss = 0.00075743\n",
      "Iteration 41, loss = 0.00074946\n",
      "Iteration 42, loss = 0.00074668\n",
      "Iteration 43, loss = 0.00073728\n",
      "Iteration 44, loss = 0.00073419\n",
      "Iteration 45, loss = 0.00072920\n",
      "Iteration 46, loss = 0.00072333\n",
      "Iteration 47, loss = 0.00071871\n",
      "Iteration 48, loss = 0.00071253\n",
      "Iteration 49, loss = 0.00070717\n",
      "Iteration 50, loss = 0.00070098\n",
      "Iteration 51, loss = 0.00069840\n",
      "Iteration 52, loss = 0.00069248\n",
      "Iteration 53, loss = 0.00068809\n",
      "Iteration 54, loss = 0.00068253\n",
      "Iteration 55, loss = 0.00068066\n",
      "Iteration 56, loss = 0.00067665\n",
      "Iteration 57, loss = 0.00067428\n",
      "Iteration 58, loss = 0.00067057\n",
      "Iteration 59, loss = 0.00066713\n",
      "Iteration 60, loss = 0.00066646\n",
      "Iteration 61, loss = 0.00066049\n",
      "Iteration 62, loss = 0.00065945\n",
      "Iteration 63, loss = 0.00065656\n",
      "Iteration 64, loss = 0.00065093\n",
      "Iteration 65, loss = 0.00064998\n",
      "Iteration 66, loss = 0.00064603\n",
      "Iteration 67, loss = 0.00064406\n",
      "Iteration 68, loss = 0.00064028\n",
      "Iteration 69, loss = 0.00064021\n",
      "Iteration 70, loss = 0.00063580\n",
      "Iteration 71, loss = 0.00063466\n",
      "Iteration 72, loss = 0.00063206\n",
      "Iteration 73, loss = 0.00062916\n",
      "Iteration 74, loss = 0.00062682\n",
      "Iteration 75, loss = 0.00062598\n",
      "Iteration 76, loss = 0.00062133\n",
      "Iteration 77, loss = 0.00061882\n",
      "Iteration 78, loss = 0.00061760\n",
      "Iteration 79, loss = 0.00061363\n",
      "Iteration 80, loss = 0.00061151\n",
      "Iteration 81, loss = 0.00060934\n",
      "Iteration 82, loss = 0.00060748\n",
      "Iteration 83, loss = 0.00060871\n",
      "Iteration 84, loss = 0.00060217\n",
      "Iteration 85, loss = 0.00060358\n",
      "Iteration 86, loss = 0.00059913\n",
      "Iteration 87, loss = 0.00059828\n",
      "Iteration 88, loss = 0.00059827\n",
      "Iteration 89, loss = 0.00059493\n",
      "Iteration 90, loss = 0.00059297\n",
      "Iteration 91, loss = 0.00059334\n",
      "Iteration 92, loss = 0.00058974\n",
      "Iteration 93, loss = 0.00058891\n",
      "Iteration 94, loss = 0.00058916\n",
      "Iteration 95, loss = 0.00058400\n",
      "Iteration 96, loss = 0.00058644\n",
      "Iteration 97, loss = 0.00058134\n",
      "Iteration 98, loss = 0.00058548\n",
      "Iteration 99, loss = 0.00058228\n",
      "Iteration 100, loss = 0.00058441\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 24): 0.9966993417105994\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.02485426668845564\n",
      "MSE (recomputed with last layer only): 0.02485426669216153\n",
      "d=25\n",
      "phi max norm: 4.18\n",
      "theta norm: 2.0\n",
      "MSE (mu): 0.024854266965802358\n",
      "mu: max 1.4910732507705688 - min -1.6650809049606323\n",
      "gap max: 2.7209904\n",
      "gap min: 2.0861626e-06\n",
      "# contexts with gap_min > 0.001: 18935\n",
      "# contexts with gap_min > 0.01: 16876\n",
      "# contexts with gap_min > 0.1: 5215\n",
      "Spanning R^20\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 22]\n",
      "Iteration 1, loss = 0.01334371\n",
      "Iteration 2, loss = 0.00268770\n",
      "Iteration 3, loss = 0.00192511\n",
      "Iteration 4, loss = 0.00161520\n",
      "Iteration 5, loss = 0.00144713\n",
      "Iteration 6, loss = 0.00135260\n",
      "Iteration 7, loss = 0.00128504\n",
      "Iteration 8, loss = 0.00125089\n",
      "Iteration 9, loss = 0.00120690\n",
      "Iteration 10, loss = 0.00118152\n",
      "Iteration 11, loss = 0.00115401\n",
      "Iteration 12, loss = 0.00113534\n",
      "Iteration 13, loss = 0.00111169\n",
      "Iteration 14, loss = 0.00110242\n",
      "Iteration 15, loss = 0.00108712\n",
      "Iteration 16, loss = 0.00107405\n",
      "Iteration 17, loss = 0.00106534\n",
      "Iteration 18, loss = 0.00104969\n",
      "Iteration 19, loss = 0.00103968\n",
      "Iteration 20, loss = 0.00102643\n",
      "Iteration 21, loss = 0.00101487\n",
      "Iteration 22, loss = 0.00100370\n",
      "Iteration 23, loss = 0.00099403\n",
      "Iteration 24, loss = 0.00098890\n",
      "Iteration 25, loss = 0.00097724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.00096781\n",
      "Iteration 27, loss = 0.00095465\n",
      "Iteration 28, loss = 0.00095246\n",
      "Iteration 29, loss = 0.00094195\n",
      "Iteration 30, loss = 0.00093470\n",
      "Iteration 31, loss = 0.00092760\n",
      "Iteration 32, loss = 0.00091771\n",
      "Iteration 33, loss = 0.00091223\n",
      "Iteration 34, loss = 0.00090777\n",
      "Iteration 35, loss = 0.00089805\n",
      "Iteration 36, loss = 0.00089309\n",
      "Iteration 37, loss = 0.00088592\n",
      "Iteration 38, loss = 0.00087947\n",
      "Iteration 39, loss = 0.00087790\n",
      "Iteration 40, loss = 0.00087034\n",
      "Iteration 41, loss = 0.00086290\n",
      "Iteration 42, loss = 0.00085806\n",
      "Iteration 43, loss = 0.00085196\n",
      "Iteration 44, loss = 0.00084674\n",
      "Iteration 45, loss = 0.00084231\n",
      "Iteration 46, loss = 0.00083883\n",
      "Iteration 47, loss = 0.00083282\n",
      "Iteration 48, loss = 0.00082589\n",
      "Iteration 49, loss = 0.00081850\n",
      "Iteration 50, loss = 0.00081734\n",
      "Iteration 51, loss = 0.00081276\n",
      "Iteration 52, loss = 0.00080870\n",
      "Iteration 53, loss = 0.00080194\n",
      "Iteration 54, loss = 0.00079739\n",
      "Iteration 55, loss = 0.00079421\n",
      "Iteration 56, loss = 0.00078671\n",
      "Iteration 57, loss = 0.00078272\n",
      "Iteration 58, loss = 0.00077616\n",
      "Iteration 59, loss = 0.00077227\n",
      "Iteration 60, loss = 0.00076707\n",
      "Iteration 61, loss = 0.00075982\n",
      "Iteration 62, loss = 0.00075390\n",
      "Iteration 63, loss = 0.00074582\n",
      "Iteration 64, loss = 0.00074612\n",
      "Iteration 65, loss = 0.00074106\n",
      "Iteration 66, loss = 0.00073813\n",
      "Iteration 67, loss = 0.00073280\n",
      "Iteration 68, loss = 0.00072893\n",
      "Iteration 69, loss = 0.00072786\n",
      "Iteration 70, loss = 0.00072157\n",
      "Iteration 71, loss = 0.00071998\n",
      "Iteration 72, loss = 0.00071569\n",
      "Iteration 73, loss = 0.00071204\n",
      "Iteration 74, loss = 0.00070991\n",
      "Iteration 75, loss = 0.00070751\n",
      "Iteration 76, loss = 0.00070135\n",
      "Iteration 77, loss = 0.00069874\n",
      "Iteration 78, loss = 0.00069504\n",
      "Iteration 79, loss = 0.00069115\n",
      "Iteration 80, loss = 0.00068948\n",
      "Iteration 81, loss = 0.00068693\n",
      "Iteration 82, loss = 0.00068330\n",
      "Iteration 83, loss = 0.00067980\n",
      "Iteration 84, loss = 0.00067743\n",
      "Iteration 85, loss = 0.00067432\n",
      "Iteration 86, loss = 0.00067457\n",
      "Iteration 87, loss = 0.00066848\n",
      "Iteration 88, loss = 0.00066459\n",
      "Iteration 89, loss = 0.00066657\n",
      "Iteration 90, loss = 0.00066039\n",
      "Iteration 91, loss = 0.00065775\n",
      "Iteration 92, loss = 0.00065576\n",
      "Iteration 93, loss = 0.00065315\n",
      "Iteration 94, loss = 0.00065103\n",
      "Iteration 95, loss = 0.00065226\n",
      "Iteration 96, loss = 0.00065063\n",
      "Iteration 97, loss = 0.00064843\n",
      "Iteration 98, loss = 0.00064467\n",
      "Iteration 99, loss = 0.00064221\n",
      "Iteration 100, loss = 0.00064081\n",
      "Iteration 101, loss = 0.00063771\n",
      "Iteration 102, loss = 0.00063712\n",
      "Iteration 103, loss = 0.00063362\n",
      "Iteration 104, loss = 0.00063250\n",
      "Iteration 105, loss = 0.00062901\n",
      "Iteration 106, loss = 0.00062941\n",
      "Iteration 107, loss = 0.00062441\n",
      "Iteration 108, loss = 0.00062542\n",
      "Iteration 109, loss = 0.00062176\n",
      "Iteration 110, loss = 0.00062138\n",
      "Iteration 111, loss = 0.00061736\n",
      "Iteration 112, loss = 0.00061646\n",
      "Iteration 113, loss = 0.00061446\n",
      "Iteration 114, loss = 0.00061221\n",
      "Iteration 115, loss = 0.00061075\n",
      "Iteration 116, loss = 0.00060830\n",
      "Iteration 117, loss = 0.00060501\n",
      "Iteration 118, loss = 0.00060632\n",
      "Iteration 119, loss = 0.00060463\n",
      "Iteration 120, loss = 0.00060306\n",
      "Iteration 121, loss = 0.00060110\n",
      "Iteration 122, loss = 0.00059792\n",
      "Iteration 123, loss = 0.00059685\n",
      "Iteration 124, loss = 0.00059705\n",
      "Iteration 125, loss = 0.00059660\n",
      "Iteration 126, loss = 0.00059349\n",
      "Iteration 127, loss = 0.00059363\n",
      "Iteration 128, loss = 0.00059160\n",
      "Iteration 129, loss = 0.00058924\n",
      "Iteration 130, loss = 0.00058829\n",
      "Iteration 131, loss = 0.00058697\n",
      "Iteration 132, loss = 0.00058718\n",
      "Iteration 133, loss = 0.00058678\n",
      "Iteration 134, loss = 0.00058534\n",
      "Iteration 135, loss = 0.00058623\n",
      "Iteration 136, loss = 0.00058285\n",
      "Iteration 137, loss = 0.00058268\n",
      "Iteration 138, loss = 0.00058093\n",
      "Iteration 139, loss = 0.00058188\n",
      "Iteration 140, loss = 0.00057935\n",
      "Iteration 141, loss = 0.00058010\n",
      "Iteration 142, loss = 0.00057947\n",
      "Iteration 143, loss = 0.00057856\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 22): 0.9969469004591248\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.02492966642710858\n",
      "MSE (recomputed with last layer only): 0.02492966642137638\n",
      "d=23\n",
      "phi max norm: 5.13\n",
      "theta norm: 2.1\n",
      "MSE (mu): 0.02492966610617146\n",
      "mu: max 1.5051709413528442 - min -1.555797815322876\n",
      "gap max: 2.6424298\n",
      "gap min: 4.7385693e-06\n",
      "# contexts with gap_min > 0.001: 18961\n",
      "# contexts with gap_min > 0.01: 16884\n",
      "# contexts with gap_min > 0.1: 5184\n",
      "Spanning R^16\n",
      "lambda HLS: 0.0\n",
      "\n",
      "Training NN -- Size [256, 20]\n",
      "Iteration 1, loss = 0.01210637\n",
      "Iteration 2, loss = 0.00249655\n",
      "Iteration 3, loss = 0.00181562\n",
      "Iteration 4, loss = 0.00152368\n",
      "Iteration 5, loss = 0.00137723\n",
      "Iteration 6, loss = 0.00128173\n",
      "Iteration 7, loss = 0.00121756\n",
      "Iteration 8, loss = 0.00117738\n",
      "Iteration 9, loss = 0.00113760\n",
      "Iteration 10, loss = 0.00110609\n",
      "Iteration 11, loss = 0.00108824\n",
      "Iteration 12, loss = 0.00106809\n",
      "Iteration 13, loss = 0.00104762\n",
      "Iteration 14, loss = 0.00103503\n",
      "Iteration 15, loss = 0.00102214\n",
      "Iteration 16, loss = 0.00100917\n",
      "Iteration 17, loss = 0.00099309\n",
      "Iteration 18, loss = 0.00098337\n",
      "Iteration 19, loss = 0.00097430\n",
      "Iteration 20, loss = 0.00096201\n",
      "Iteration 21, loss = 0.00095758\n",
      "Iteration 22, loss = 0.00094702\n",
      "Iteration 23, loss = 0.00093609\n",
      "Iteration 24, loss = 0.00093009\n",
      "Iteration 25, loss = 0.00091802\n",
      "Iteration 26, loss = 0.00090880\n",
      "Iteration 27, loss = 0.00090217\n",
      "Iteration 28, loss = 0.00089693\n",
      "Iteration 29, loss = 0.00088555\n",
      "Iteration 30, loss = 0.00087952\n",
      "Iteration 31, loss = 0.00087401\n",
      "Iteration 32, loss = 0.00086704\n",
      "Iteration 33, loss = 0.00085387\n",
      "Iteration 34, loss = 0.00085142\n",
      "Iteration 35, loss = 0.00084482\n",
      "Iteration 36, loss = 0.00083337\n",
      "Iteration 37, loss = 0.00082650\n",
      "Iteration 38, loss = 0.00082280\n",
      "Iteration 39, loss = 0.00081569\n",
      "Iteration 40, loss = 0.00080404\n",
      "Iteration 41, loss = 0.00080352\n",
      "Iteration 42, loss = 0.00079221\n",
      "Iteration 43, loss = 0.00078740\n",
      "Iteration 44, loss = 0.00077767\n",
      "Iteration 45, loss = 0.00077354\n",
      "Iteration 46, loss = 0.00076744\n",
      "Iteration 47, loss = 0.00075984\n",
      "Iteration 48, loss = 0.00075348\n",
      "Iteration 49, loss = 0.00075137\n",
      "Iteration 50, loss = 0.00074173\n",
      "Iteration 51, loss = 0.00074057\n",
      "Iteration 52, loss = 0.00073405\n",
      "Iteration 53, loss = 0.00073062\n",
      "Iteration 54, loss = 0.00072474\n",
      "Iteration 55, loss = 0.00071958\n",
      "Iteration 56, loss = 0.00071568\n",
      "Iteration 57, loss = 0.00071580\n",
      "Iteration 58, loss = 0.00070856\n",
      "Iteration 59, loss = 0.00070550\n",
      "Iteration 60, loss = 0.00070186\n",
      "Iteration 61, loss = 0.00069622\n",
      "Iteration 62, loss = 0.00069098\n",
      "Iteration 63, loss = 0.00069019\n",
      "Iteration 64, loss = 0.00068470\n",
      "Iteration 65, loss = 0.00068135\n",
      "Iteration 66, loss = 0.00067939\n",
      "Iteration 67, loss = 0.00067405\n",
      "Iteration 68, loss = 0.00067226\n",
      "Iteration 69, loss = 0.00066829\n",
      "Iteration 70, loss = 0.00066591\n",
      "Iteration 71, loss = 0.00066343\n",
      "Iteration 72, loss = 0.00066174\n",
      "Iteration 73, loss = 0.00065608\n",
      "Iteration 74, loss = 0.00065684\n",
      "Iteration 75, loss = 0.00065115\n",
      "Iteration 76, loss = 0.00064987\n",
      "Iteration 77, loss = 0.00064889\n",
      "Iteration 78, loss = 0.00064305\n",
      "Iteration 79, loss = 0.00064014\n",
      "Iteration 80, loss = 0.00064076\n",
      "Iteration 81, loss = 0.00063347\n",
      "Iteration 82, loss = 0.00063199\n",
      "Iteration 83, loss = 0.00063273\n",
      "Iteration 84, loss = 0.00062837\n",
      "Iteration 85, loss = 0.00062756\n",
      "Iteration 86, loss = 0.00062432\n",
      "Iteration 87, loss = 0.00062254\n",
      "Iteration 88, loss = 0.00062101\n",
      "Iteration 89, loss = 0.00061751\n",
      "Iteration 90, loss = 0.00061675\n",
      "Iteration 91, loss = 0.00061366\n",
      "Iteration 92, loss = 0.00061266\n",
      "Iteration 93, loss = 0.00061093\n",
      "Iteration 94, loss = 0.00060821\n",
      "Iteration 95, loss = 0.00061034\n",
      "Iteration 96, loss = 0.00060723\n",
      "Iteration 97, loss = 0.00060690\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "R^2 (size 20): 0.9967337496498048\n",
      "\n",
      "Saving model...\n",
      "MSE (original): 0.025029933152811924\n",
      "MSE (recomputed with last layer only): 0.025029933168712073\n",
      "d=21\n",
      "phi max norm: 4.54\n",
      "theta norm: 1.71\n",
      "MSE (mu): 0.02502993324058359\n",
      "mu: max 1.5798008441925049 - min -1.6382440328598022\n",
      "gap max: 2.728639\n",
      "gap min: 7.56979e-06\n",
      "# contexts with gap_min > 0.001: 18937\n",
      "# contexts with gap_min > 0.01: 16985\n",
      "# contexts with gap_min > 0.1: 5212\n",
      "Spanning R^17\n",
      "lambda HLS: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit smaller networks\n",
    "\n",
    "hidden_small = [256]\n",
    "ds = [30, 28, 26, 24, 22, 20]\n",
    "nets = {}\n",
    "\n",
    "# redefine targets based on the outputs of the larger network (the input is still X)\n",
    "y_mu = []\n",
    "for t in range(n_users):\n",
    "    for z in range(n_items):\n",
    "        y_mu.append(mu[t, z])\n",
    "y_mu = np.array(y_mu)\n",
    "\n",
    "del(mu)\n",
    "del(net)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_mu, test_size=test_size)\n",
    "\n",
    "for j in ds:\n",
    "    size = hidden_small + [j]\n",
    "    print(\"Training NN -- Size {0}\".format(size))\n",
    "    nets[j] = MLPRegressor(hidden_layer_sizes=size, max_iter=500, tol=1e-6, verbose=True).fit(X, y_mu)\n",
    "    print(\"R^2 (size {0}): {1}\".format(j, nets[j].score(X, y_mu)))\n",
    "    print()\n",
    "    print(\"Saving model...\")\n",
    "    save_model(nets[j])\n",
    "    print()\n",
    "    nets[j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
