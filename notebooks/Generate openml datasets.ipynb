{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from lrcb.sklearn import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lrcb.representations.finite_representations import LinearRepresentation, normalize_param, rank, hls_rank, hls_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch classification datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn OpenML classification problem into contextual bandit\n",
    "\n",
    "Datasets from Bietti, Alberto, Alekh Agarwal, and John Langford. \"A contextual bandit bake-off.\" arXiv preprint arXiv:1802.04064 (2018). Code https://github.com/albietz/cb_bakeoff based on Vowpal Wabbit fork https://github.com/albietz/vowpal_wabbit/tree/bakeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the OpenML ids of the original datasets selected by Bietti et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [3, 6, 8, 10, 11, 12, 14, 16, 18, 20, 21, 22, 23, 26, 28, 30, 31, 32, 36, 37, 39, 40, 41, 43, 44, 46, 48, 50, 53, 54, 59, 60, 61, 62, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 180, 181, 182, 183, 184, 187, 189, 197, 209, 223, 227, 273, 275, 276, 277, 278, 279, 285, 287, 292, 293, 294, 298, 300, 307, 310, 312, 313, 329, 333, 334, 335, 336, 337, 338, 339, 343, 346, 351, 354, 357, 375, 377, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 444, 446, 448, 450, 457, 458, 459, 461, 462, 463, 464, 465, 467, 468, 469, 472, 475, 476, 477, 478, 479, 480, 554, 679, 682, 683, 685, 694, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799, 800, 801, 803, 804, 805, 806, 807, 808, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 836, 837, 838, 841, 843, 845, 846, 847, 848, 849, 850, 851, 853, 855, 857, 859, 860, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 888, 891, 892, 893, 894, 895, 896, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934, 935, 936, 937, 938, 941, 942, 943, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 958, 959, 962, 964, 965, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 983, 987, 988, 991, 994, 995, 996, 997, 1004, 1005, 1006, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1019, 1020, 1021, 1022, 1025, 1026, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1046, 1048, 1049, 1050, 1054, 1055, 1056, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071, 1073, 1075, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1100, 1104, 1106, 1107, 1110, 1113, 1115, 1116, 1117, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1169, 1216, 1217, 1218, 1233, 1235, 1236, 1237, 1238, 1241, 1242, 1412, 1413, 1441, 1442, 1443, 1444, 1449, 1451, 1453, 1454, 1455, 1457, 1459, 1460, 1464, 1467, 1470, 1471, 1472, 1473, 1475, 1481, 1482, 1483, 1486, 1487, 1488, 1489, 1496, 1498, 1590]\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(id):\n",
    "    ds = openml.datasets.get_dataset(id)\n",
    "    X, y, _, _ = ds.get_data(target=ds.default_target_attribute)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep datasets with only real-valued features, categorical labels, less than 100k samples, and n_features <= sqrt(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "selected_ids = []\n",
    "for i in ids:\n",
    "    X, y = get_dataset(i)\n",
    "    if np.all(X.dtypes=='float64') and X.shape[0] > X.shape[1]**2 and X.shape[0]<=100000 and type(y.values)==pd.core.arrays.categorical.Categorical:\n",
    "        selected_ids.append(i)\n",
    "len(selected_ids)\n",
    "np.save('ids.npy', np.array(selected_ids, dtype=np.int16))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_ids = list(np.load('ids.npy'))\n",
    "len(selected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn classification problem into contextual bandit\n",
    "Contexts (x) are the original feature vectors of the problem (standardized)\n",
    "\n",
    "Actions (a) are class labels\n",
    "\n",
    "#### Logistic loss\n",
    "We fit a NN to predict p(a|x) given x concatenated to the one-hot encoding of a, using cross-entropy loss\n",
    "\n",
    "Training data are the original data concatenated to each action encoding. Targets are 1 for correct class and 0 otherwise\n",
    "\n",
    "The output unit of the NN is a sigmoid. The second-to-last units are our new context-action features. The score fed to the sigmoid is our reward (new ground truth). The last layer of weights are the \"unknown\" parameters of our linear reward model.\n",
    "\n",
    "#### Squared (or hinge) loss\n",
    "We fit a NN to predict 1. for correct classification and 0. for wrong given x concatenated to the one-hot enconding of a, using squared (or hinge) loss. Cf Bietti et al. (2018) Equation (6) and Section 2.4 \n",
    "\n",
    "The output unit is linear. Our features are the second-to-last units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    return scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_dataset(X,y):\n",
    "    X = standardize(X)\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    assert len(y) == n_samples\n",
    "    classes = y.values.unique()\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    new_X = np.zeros((n_samples*n_classes, n_features+n_classes))\n",
    "    new_y = np.zeros(n_samples*n_classes)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_classes):\n",
    "            one_hot = np.zeros(n_classes)\n",
    "            one_hot[j] = 1.\n",
    "            new_X[i*n_classes + j] = np.concatenate((X[i], one_hot))\n",
    "            new_y[i*n_classes + j] = 1. if y[i] == classes[j] else 0.\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(net, X, n_contexts, n_actions):\n",
    "    \n",
    "    # Build features\n",
    "    hidden_layer_sizes = list(net.hidden_layer_sizes)\n",
    "\n",
    "    layer_units = [X.shape[1]] + hidden_layer_sizes + [1]\n",
    "    activations = [X]\n",
    "    for i in range(net.n_layers_ - 1):\n",
    "        activations.append(np.empty((X.shape[0], layer_units[i + 1])))\n",
    "\n",
    "    net._forward_pass(activations)\n",
    "    y_pred = activations[-1]\n",
    "\n",
    "    # get weights\n",
    "    last_w = net.coefs_[-1]\n",
    "    bias = np.array(net.intercepts_[-1]).reshape((1, 1))\n",
    "    last_w = np.concatenate([last_w, bias])\n",
    "\n",
    "    # get last-layer features\n",
    "    last_feat = np.array(activations[-2], dtype=np.float32)\n",
    "    last_feat = np.concatenate([last_feat, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "    # get prediction\n",
    "    pred = last_feat.dot(last_w)\n",
    "\n",
    "    # get feature matrix\n",
    "    d = hidden_layer_sizes[-1] + 1\n",
    "    phi = np.empty((n_contexts, n_actions, d), dtype=np.float32)\n",
    "    idx = 0\n",
    "    for t in range(n_contexts):\n",
    "        for z in range(n_actions):\n",
    "            phi[t, z, :] = last_feat[idx, :] \n",
    "            idx += 1\n",
    "    assert idx == last_feat.shape[0]\n",
    "\n",
    "    # get param\n",
    "    theta = np.array(last_w, dtype=np.float32).squeeze()\n",
    "\n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_representation(X, y, test_size=0.25, hidden=(32,32), max_iter=500, normalize=True, \n",
    "                         loss='log_loss', alpha=0.0001):\n",
    "    n_contexts = X.shape[0]\n",
    "    n_actions = len(y.values.unique())\n",
    "    \n",
    "    #Preprocess dataset \n",
    "    X_rep, y_rep = representation_dataset(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_rep, y_rep, test_size=test_size)\n",
    "    \n",
    "    #Fit NN\n",
    "    if loss=='squared':\n",
    "        net = MLPRegressor(hidden_layer_sizes=hidden, max_iter=max_iter,\n",
    "                           verbose=False, alpha=alpha)\n",
    "    elif loss=='hinge':\n",
    "        net = MLPRegressor(hidden_layer_sizes=hidden, max_iter=max_iter, \n",
    "                           loss='hinge_loss',\n",
    "                           verbose=False, alpha=alpha)\n",
    "    elif loss=='logistic':\n",
    "        net = MLPClassifier(hidden_layer_sizes=hidden, max_iter=max_iter, verbose=False, alpha=alpha)\n",
    "    net.fit(X_train, y_train)\n",
    "    score = net.score(X_test, y_test)\n",
    "    \n",
    "    #Build representation\n",
    "    phi, theta = build_model(net, X_rep, n_contexts, n_actions)\n",
    "    rep = LinearRepresentation(phi, theta)\n",
    "    if normalize:\n",
    "        rep = normalize_param(rep)\n",
    "    return rep, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: accuracy=0.842832, rank=33, hls_rank=32, hls_lambda=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/lrcb/lrcb/sklearn/mlp.py:625: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8a43a3834205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d: accuracy=%f, rank=%d, hls_rank=%d, hls_lambda=%f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhls_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhls_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lrcb/lrcb/representations/finite_representations.py\u001b[0m in \u001b[0;36mhls_lambda\u001b[0;34m(rep, cprobs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhls_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mmineig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_eig_outer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimal_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_contexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lrcb/lrcb/utils.py\u001b[0m in \u001b[0;36mmin_eig_outer\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\"\"\"Computes minimum eigenvalue of AA^T\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmin_eig_outer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/robot/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "test_size=0.25\n",
    "hidden=(32,32)\n",
    "max_iter=1000\n",
    "normalize=True #normalize parameters of linear combination\n",
    "tol = 1e-12\n",
    "loss='squared'#'logistic', 'squared' or 'hinge'\n",
    "reg = 0.0001 #sklearn default: 0.0001 \n",
    "\n",
    "for i in selected_ids[0:10]:\n",
    "    X, y = get_dataset(i)\n",
    "    rep, score = learn_representation(X, y, test_size, hidden, max_iter, normalize, loss=loss, alpha=reg)\n",
    "    print(\"%d: accuracy=%f, rank=%d, hls_rank=%d, hls_lambda=%f\" % (i, score, rank(rep,tol), hls_rank(rep,tol), hls_lambda(rep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
